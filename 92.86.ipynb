{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это решение берёт ориентировочно 92.8 на Public LeaderBoard (5 место на текущий момент)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что было опробовано и не зажгло:\n",
    "- Стакинг моделей. Был испробован стакинг сильной модели (Random Forest) и логистической регресси + KNN. Мета классифиикатором был выбран катбуст с категориальными фичами (месяц, день недели, субъект РФ, область РФ).\n",
    "- Были опробованы счётчики событий в радиусе с течением времени - к сожалению, очень сильно дропнули валидацию...\n",
    "- Были опробованы счётчики событий в субъекте, области, номере (поделил всю РФ на сетку) ячейки в прошлом году в том же месяце.\n",
    "- Нейронная сеть (полагаю, что это из-за разнородности данных)\n",
    "\n",
    "Что в свою очеред зажгло:\n",
    "- Категориальные фичи в катбусте (месяц, день недели, субъект РФ и область)\n",
    "- Минимальное расстояние до пожара определёного типа\n",
    "- 20-е по удалённости значение расстояния до каждого типа пожаров (таким образом показывается компактность каждого типа пожаров).\n",
    "- Также хорошей идеей стало взятие ncep данных на разных высотах (средняя высота над уровнем моря в России 150 метров, тогда как в бейзлайне были использованы только данные на высоте 1000 метров) + хорошей идеей стало взятие vwind данных из ncep.\n",
    "- Также зажгли счётчики по количеству событий в субъекте, области и немере ячейки за последний месяц (к сожалению, в итоговой модели их использовать нельзя, так как учился я на анных до апреля, а на тесте будет декабрь без возможности дообучения :( ) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заимпортим все необходимые библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from datetime import date\n",
    "import xarray\n",
    "import requests\n",
    "import datetime\n",
    "import numpy as np\n",
    "import gzip\n",
    "from tqdm import tqdm as tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from math import cos, asin, sqrt\n",
    "import pandas as pd\n",
    "import warnings \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import datetime\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from multiprocessing import Process\n",
    "import multiprocessing\n",
    "import mlens\n",
    "from mlens.ensemble import SuperLearner\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "height_surf = xarray.open_dataset(\"./data/hgt.sfc.nc\")\n",
    "land = xarray.open_dataset(\"./data/land.nc\")\n",
    "nceps = [[], [], [], [], [], [], [], []]\n",
    "nceps_sfc = [[], [], [], [], [], [], [], []]\n",
    "years = [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "\n",
    "for index, year in enumerate(years):\n",
    "    for var in ('hgt', 'srfpt', 'trpp', 'trpt', 'air', 'uwnd', 'vwnd', 'rhum'):\n",
    "        dataset_filename = './data/{}.{}.nc'.format(var, year)\n",
    "        nceps[index].append(xarray.open_dataset(dataset_filename))\n",
    "    nceps[index] = xarray.merge(nceps[index])\n",
    "    \n",
    "for index, year in enumerate(years):\n",
    "    for var in ('air.sfc', 'rhum.sfc', 'vwnd.sfc'):\n",
    "        dataset_filename = './data/{}.{}.nc'.format(var, year)\n",
    "        nceps_sfc[index].append(xarray.open_dataset(dataset_filename))\n",
    "    nceps_sfc[index] = xarray.merge(nceps_sfc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и говорилось выше, давайте смотреть данные ncep на разных высотах. Так как вычисление этих фичей занимает очень большое время - внутри предлагается это распараллелеить. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pandas.read_csv('./wildfires-data-train/wildfires_train.csv')\n",
    "    \n",
    "def extract_features(row, index):\n",
    "    date = row['date']\n",
    "    dt = datetime.date(int(date[:4]), int(date[5:7]), int(date[8:10]))\n",
    "    levels = [100, 150, 200, 500, 1000]\n",
    "    answer = {\n",
    "        'fire_id': row['fire_id'],\n",
    "        'month': dt.month,\n",
    "        'day': dt.day,\n",
    "        'fire_type': row['fire_type'],\n",
    "        'fire_type_name': row['fire_type_name'],\n",
    "        'longitude': row['longitude'],\n",
    "        'latitude': row['latitude'],\n",
    "        'date': row['date'],\n",
    "        'weekday': dt.weekday()}\n",
    "    \n",
    "    for level in levels:\n",
    "        point = nceps[index].sel(\n",
    "            lon=row['longitude'],\n",
    "            lat=row['latitude'],\n",
    "            level=level,\n",
    "            method='nearest',\n",
    "        )\n",
    "        \n",
    "        p3w = point.rolling(time=21).mean()\n",
    "        p1w = point.rolling(time=7).mean()\n",
    "        p2w = point.rolling(time=14).mean()\n",
    "\n",
    "        v = point.sel(time=date)\n",
    "        v1w = p1w.sel(time=date)\n",
    "        v2w = p2w.sel(time=date)\n",
    "        v3w = p3w.sel(time=date)\n",
    "        \n",
    "        answer['temperature ' + str(level)] = v.air.values.item(0)\n",
    "        answer['humidity ' + str(level)] = v.rhum.values.item(0)\n",
    "        answer['uwind ' + str(level)] = v.uwnd.values.item(0)\n",
    "        answer['temperature_1w_mean ' + str(level)] = v1w.air.values.item(0)\n",
    "        answer['temperature_2w_mean ' + str(level)] = v2w.air.values.item(0)\n",
    "        answer['temperature_3w_mean ' + str(level)] = v3w.air.values.item(0)\n",
    "        answer['humidity_1w_mean ' + str(level)] = v1w.rhum.values.item(0)\n",
    "        answer['humidity_2w_mean ' + str(level)] = v2w.rhum.values.item(0)\n",
    "        answer['humidity_3w_mean ' + str(level)] = v3w.rhum.values.item(0)\n",
    "        answer['height_1w_mean' + str(level)] = v1w.hgt.values.item(0)\n",
    "        answer['height_2w_mean' + str(level)] = v2w.hgt.values.item(0)\n",
    "        answer['height_3w_mean' + str(level)] = v3w.hgt.values.item(0)\n",
    "        answer['srfpt_1w_mean' + str(level)] = v1w.srfpt.values.item(0)\n",
    "        answer['srfpt_2w_mean' + str(level)] = v2w.srfpt.values.item(0)\n",
    "        answer['srfpt_3w_mean' + str(level)] = v3w.srfpt.values.item(0)\n",
    "        answer['trpp_1w_mean' + str(level)] = v1w.trpp.values.item(0)\n",
    "        answer['trpp_2w_mean' + str(level)] = v2w.trpp.values.item(0)\n",
    "        answer['trpp_3w_mean' + str(level)] = v3w.trpp.values.item(0)\n",
    "        answer['trpt_1w_mean' + str(level)] = v1w.trpt.values.item(0)\n",
    "        answer['trpt_2w_mean' + str(level)] = v2w.trpt.values.item(0)\n",
    "        answer['trpt_3w_mean' + str(level)] = v3w.trpt.values.item(0)\n",
    "        answer['vwnd_1w_mean' + str(level)] = v1w.vwnd.values.item(0)\n",
    "        answer['vwnd_2w_mean' + str(level)] = v2w.vwnd.values.item(0)\n",
    "        answer['vwnd_3w_mean' + str(level)] = v3w.vwnd.values.item(0)\n",
    "        answer['uwnd_1w_mean' + str(level)] = v1w.uwnd.values.item(0)\n",
    "        answer['uwnd_2w_mean' + str(level)] = v2w.uwnd.values.item(0)\n",
    "        answer['uwnd_3w_mean' + str(level)] = v3w.uwnd.values.item(0)\n",
    "        \n",
    "    point = nceps_sfc[index].sel(\n",
    "        lon=row['longitude'],\n",
    "        lat=row['latitude'],\n",
    "        method='nearest',\n",
    "    )\n",
    "\n",
    "    p3w = point.rolling(time=21).mean()\n",
    "    p1w = point.rolling(time=7).mean()\n",
    "    p2w = point.rolling(time=14).mean()\n",
    "\n",
    "    v = point.sel(time=date)\n",
    "    v1w = p1w.sel(time=date)\n",
    "    v2w = p2w.sel(time=date)\n",
    "    v3w = p3w.sel(time=date)\n",
    "\n",
    "    answer['temperature_sfc'] = v.air.values.item(0)\n",
    "    answer['humidity_sfc'] = v.rhum.values.item(0)\n",
    "    answer['temperature_1w_mean_sfc'] = v1w.air.values.item(0)\n",
    "    answer['temperature_2w_mean_sfc'] = v2w.air.values.item(0)\n",
    "    answer['temperature_3w_mean_sfc'] = v3w.air.values.item(0)\n",
    "    answer['vwnd_1w_mean_sfc'] = v1w.vwnd.values.item(0)\n",
    "    answer['vwnd_2w_mean_sfc'] = v2w.vwnd.values.item(0)\n",
    "    answer['vwnd_3w_mean_sfc'] = v3w.vwnd.values.item(0)\n",
    "    answer['humidity_1w_mean_sfc'] = v1w.rhum.values.item(0)\n",
    "    answer['humidity_2w_mean_sfc'] = v2w.rhum.values.item(0)\n",
    "    answer['humidity_3w_mean_sfc'] = v3w.rhum.values.item(0)\n",
    "        \n",
    "    return answer\n",
    "\n",
    "def get_interval_features(query, index, features_list):\n",
    "    df_subsample = df_train.query(query)\n",
    "    df_features = []\n",
    "    for i, row in tqdm(df_subsample.iterrows(), total=df_subsample.shape[0]):\n",
    "        features = extract_features(row, index)\n",
    "        df_features.append(features)\n",
    "    df_features = pandas.DataFrame(df_features).sort_values('date')\n",
    "    df_features.set_index('fire_id', inplace=True)\n",
    "    features_list[index] = df_features\n",
    "\n",
    "def extract_all_basic_features(queries):\n",
    "    manager = multiprocessing.Manager()\n",
    "    return_dict = manager.dict()\n",
    "    pool = []\n",
    "    \n",
    "    for index, query in enumerate(queries):\n",
    "        pool.append(Process(target=get_interval_features, args=(query, index, return_dict)))\n",
    "        pool[index].start()\n",
    "\n",
    "    for thread in pool:\n",
    "        thread.join()\n",
    "    \n",
    "    return return_dict\n",
    "\n",
    "# query1213 = '(date > \"2012\") & (date < \"2013\")' #распределение сильно отличается\n",
    "# query1314 = '(date > \"2013\") & (date < \"2014\")'\n",
    "# query1415 = '(date > \"2014\") & (date < \"2015\")'\n",
    "# query1516 = '(date > \"2015\") & (date < \"2016\")'\n",
    "# query1617 = '(date > \"2016\") & (date < \"2017\")'\n",
    "# query1718 = '(date > \"2017\") & (date < \"2018\")'\n",
    "# query1819 = '(date > \"2018\") & (date < \"2019\")'\n",
    "# query1920 = '(date > \"2019\") & (date < \"2020\")'\n",
    "\n",
    "# queries = [query1213, query1314, query1415, query1516, query1617, query1718, query1819, query1920]\n",
    "# queries_titles = ['2012-2013', '2013-2014', '2014-2015', '2015-2016',\n",
    "#                   '2016-2017', '2017-2018', '2018-2019', '2019-2020']\n",
    "\n",
    "# years = ['2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019']\n",
    "\n",
    "# features_list = extract_all_basic_features(queries).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень важным шагом был сбор данных о крупнейших российских городах (областях и субъектах, в которых они находятся).\n",
    "Данных файл будет приложен в репозитории. Также в в ноутбуке EDA будет код, для качивания фотографий со спутника для наглядного \n",
    "представления территорий, на которых был пожар, на нх будет очень хорошо видно, какие территории покрыты чисто лесом, а какие\n",
    "покрыты полями, что сильно поможет различать контролируемый пал / неконтролируемый пал от лесного и природного подаров.\n",
    "Для классификации типа местности на изображении была использована нейронная сеть ResNet50 (ноутбук появится позже).\n",
    "Далее всё это было переведено в файлы field_cords.csv и forest_coords.csv. Стоит заметить что изображения экстрактились\n",
    "с шагом примерно 0.2 градуса по широте и долготе. Это очень помогло модели различать типы пожаров. В коде ниже мы просто считываем\n",
    "уже подготовленные данные из файлов, которые можно будет найти в данном репозитории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_info = pd.read_json('./data/russian-cities.json')\n",
    "cities_info['latitude'] = cities_info.apply(lambda x: x.coords['lat'], axis=1)\n",
    "cities_info['longitude'] = cities_info.apply(lambda x: x.coords['lon'], axis=1)\n",
    "cities_info = cities_info.drop(columns=['coords'])\n",
    "cities_info['subject'] = cities_info['subject'].astype('category').cat.codes\n",
    "cities_info['district'] = cities_info['district'].astype('category').cat.codes\n",
    "\n",
    "forest_info = pd.read_csv('forest_coords.csv')\n",
    "field_info = pd.read_csv('field_coords.csv')\n",
    "field_info['field'] = field_info['field'].apply(lambda x: 2)\n",
    "merged_land_df = pd.concat([forest_info.rename({'forest': 'land_indx'}, axis=1),\n",
    "                            field_info.rename({'field': 'land_indx'}, axis=1)]).reset_index()\n",
    "forests_df = pd.read_csv('nature_forests.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самые основные фнкции для feature generation на основе KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_subj_month_only_prev(df_train, df_test, field_type='subject'):\n",
    "    df_train_dt = df_train.copy()\n",
    "    df_test_dt = df_test.copy()\n",
    "    merged_df = pd.concat([df_train, df_test]).reset_index(drop=True)\n",
    "    \n",
    "    date_area_count = merged_df.groupby(['date', field_type]).count().reset_index()\n",
    "    df_area_counters = pd.DataFrame(columns=['events_amount_passed_in_' + str(field_type)])\n",
    "    for index, row in tqdm(date_area_count.iterrows()):\n",
    "        year = int(row.date[:4])\n",
    "        month = int(row.date[5:7])\n",
    "        day = row.date[8:]\n",
    "        if month == 1: \n",
    "            prev_month = \"12\"\n",
    "            prev_year = str(year - 1)\n",
    "        else:\n",
    "            prev_month = str(month - 1)\n",
    "            if len(prev_month) == 1:\n",
    "                prev_month = \"0\" + prev_month\n",
    "            prev_year = str(year)\n",
    "        amount_events = date_area_count[(date_area_count[field_type] == row[field_type]) &\n",
    "                      (date_area_count.date >= str(prev_year) + \"-\" + str(prev_month) + str(day)) &\n",
    "                      (date_area_count.date <= row.date)].month.sum()\n",
    "        \n",
    "        df_area_counters = df_area_counters.append(pd.DataFrame(np.array(amount_events).reshape(-1, 1), \n",
    "                                                                columns=['events_amount_passed_last_month_in_' + str(field_type)]))\n",
    "        \n",
    "    df_area_counters = df_area_counters.reset_index(drop=True)\n",
    "    \n",
    "    df_area_counters = pd.concat([df_area_counters, date_area_count], axis=1)[['date', field_type, \n",
    "                                                                          'events_amount_passed_last_month_in_' + str(field_type)]]\n",
    "    df_train_dt = df_train_dt.merge(df_area_counters, how='left', on=['date', field_type])\n",
    "    df_test_dt = df_test_dt.merge(df_area_counters, how='left', on=['date', field_type])\n",
    "    \n",
    "    return df_train_dt, df_test_dt\n",
    "\n",
    "def get_count_subj_month(df_train, df_test, field_type='subject'):\n",
    "    df_train_dt = df_train.copy()\n",
    "    df_test_dt = df_test.copy()\n",
    "    merged_df = pd.concat([df_train, df_test]).reset_index(drop=True)\n",
    "    \n",
    "    date_area_count = merged_df.groupby(['date', field_type]).count().reset_index()\n",
    "    df_area_counters = pd.DataFrame(columns=['events_amount_passed_in_' + str(field_type)])\n",
    "    area_count = {}\n",
    "    for index, row in date_area_count.iterrows():\n",
    "        if row[field_type] in area_count.keys():\n",
    "            area_count[row[field_type]] += row['month']\n",
    "        else:\n",
    "            area_count[row[field_type]] = row['month']\n",
    "        df_area_counters = df_area_counters.append(pd.DataFrame([area_count[row[field_type]]], \n",
    "                                                                columns=['events_amount_passed_in_' + str(field_type)]))\n",
    "        \n",
    "    df_area_counters = df_area_counters.reset_index(drop=True)\n",
    "    \n",
    "    df_area_counters = pd.concat([df_area_counters, date_area_count], axis=1)[['date', field_type, \n",
    "                                                                          'events_amount_passed_in_' + str(field_type)]]\n",
    "    df_train_dt = df_train_dt.merge(df_area_counters, how='left', on=['date', field_type])\n",
    "    df_test_dt = df_test_dt.merge(df_area_counters, how='left', on=['date', field_type])\n",
    "    \n",
    "    return df_train_dt, df_test_dt\n",
    "\n",
    "def get_radius_counters(df_train, df_test, fire_types, radiuses=[1.5]):\n",
    "    all_answers_train, all_answers_test = [], []\n",
    "    all_min_dist_train, all_min_dist_test = np.full((df_train.shape[0], len(fire_types)), 1000.0).astype(float),  np.full((df_test.shape[0], len(fire_types)), 1000).astype(float)\n",
    "    all_max_dist_train, all_max_dist_test = np.full((df_train.shape[0], len(fire_types)), 1000.0).astype(float),  np.full((df_test.shape[0], len(fire_types)), 1000).astype(float)\n",
    "    \n",
    "    for radius in tqdm(radiuses):\n",
    "        answers_train = np.full((df_train.shape[0], len(fire_types)), 1000.0).astype(float)\n",
    "        answers_test = np.full((df_test.shape[0], len(fire_types)), 1000.0).astype(float)\n",
    "        answers_min_dist_train = np.full((df_train.shape[0], len(fire_types)), 1000.0).astype(float)\n",
    "        answers_min_dist_test = np.full((df_test.shape[0], len(fire_types)), 1000.0).astype(float)\n",
    "        for fire_index, fire_type in enumerate(fire_types):\n",
    "            neigh = NearestNeighbors(2, radius=radius)\n",
    "            neigh.fit(df_train[df_train.fire_type_name == fire_type][['longitude', 'latitude']])\n",
    "            \n",
    "            dist_train, indices_train = neigh.radius_neighbors(df_train[['longitude', 'latitude']])\n",
    "            dist_test, indices_test = neigh.radius_neighbors(df_test[['longitude', 'latitude']])\n",
    "            dist_min_train, _ = neigh.kneighbors(df_train[['longitude', 'latitude']])\n",
    "            dist_min_test, _ = neigh.kneighbors(df_test[['longitude', 'latitude']])\n",
    "            \n",
    "            for indx, val in enumerate(dist_train):\n",
    "                ans = len(val)\n",
    "                if val.shape[0] > 0 and val.min() < 1e-9:\n",
    "                    ans -= 1\n",
    "                if dist_min_train[indx].min() < 1e-9:\n",
    "                    answers_min_dist_train[indx][fire_index] = dist_min_train[indx].max()\n",
    "                else:\n",
    "                    answers_min_dist_train[indx][fire_index] = dist_min_train[indx].min()\n",
    "                answers_train[indx][fire_index] = ans\n",
    "            for indx, val in enumerate(dist_test):\n",
    "                ans = len(val)\n",
    "                if val.shape[0] > 0 and val.min() < 1e-9:\n",
    "                    ans -= 1\n",
    "                if dist_min_test[indx].min() < 1e-9:\n",
    "                    answers_min_dist_test[indx][fire_index] = dist_min_test[indx].max()\n",
    "                else:\n",
    "                    answers_min_dist_test[indx][fire_index] = dist_min_test[indx].min()\n",
    "                answers_test[indx][fire_index] = ans\n",
    "                \n",
    "        all_answers_train.append(answers_train.astype(int))\n",
    "        all_answers_test.append(answers_test.astype(int))\n",
    "        all_min_dist_train = answers_min_dist_train\n",
    "        all_min_dist_test = answers_min_dist_test\n",
    "    \n",
    "    for fire_index, fire_type in tqdm(enumerate(fire_types)):\n",
    "        neigh = NearestNeighbors(20)\n",
    "        neigh.fit(df_train[df_train.fire_type_name == fire_type][['longitude', 'latitude']])\n",
    "\n",
    "        dist_min_train, _ = neigh.kneighbors(df_train[['longitude', 'latitude']])\n",
    "        dist_min_test, _ = neigh.kneighbors(df_test[['longitude', 'latitude']])\n",
    "\n",
    "        for indx, val in enumerate(dist_min_train):\n",
    "            all_max_dist_train[indx][fire_index] = dist_min_train[indx].max()\n",
    "        for indx, val in enumerate(dist_min_test):\n",
    "            all_max_dist_test[indx][fire_index] = dist_min_test[indx].max()\n",
    "            \n",
    "    return all_answers_train, all_answers_test, all_min_dist_train, all_min_dist_test, all_max_dist_train, all_max_dist_test\n",
    "\n",
    "def get_radius_cities_amount(df_train, df_test, nearest_cities, radiuses=[1.5]):\n",
    "    all_answers_train, all_answers_test = [], []\n",
    "    all_districts_train, all_districts_test = [], []\n",
    "    all_subj_train, all_subj_test = [], []\n",
    "    min_dist_train, min_dist_test = np.full((df_train.shape[0], 1), 1000.0).astype(float), np.full((df_test.shape[0], 1), 1000.0).astype(float)\n",
    "    \n",
    "    for radius in tqdm(radiuses):\n",
    "        answers_train = np.full((df_train.shape[0], 1), 1000.0).astype(float)\n",
    "        answers_test = np.full((df_test.shape[0], 1), 1000.0).astype(float)\n",
    "        \n",
    "        neigh = NearestNeighbors(10, radius=radius)\n",
    "        neigh.fit(nearest_cities[['longitude', 'latitude']])\n",
    "        dist_train, indices_train = neigh.radius_neighbors(df_train[['longitude', 'latitude']])\n",
    "        dist_test, indices_test = neigh.radius_neighbors(df_test[['longitude', 'latitude']])\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors=1)\n",
    "        knn.fit(nearest_cities[['longitude', 'latitude']], nearest_cities['subject'])\n",
    "        subject_train = knn.predict(df_train[['longitude', 'latitude']])\n",
    "        subject_test = knn.predict(df_test[['longitude', 'latitude']])\n",
    "        \n",
    "        knn.fit(nearest_cities[['longitude', 'latitude']], nearest_cities['district'])\n",
    "        district_train = knn.predict(df_train[['longitude', 'latitude']])\n",
    "        district_test = knn.predict(df_test[['longitude', 'latitude']])\n",
    "        \n",
    "        neigh_inf = NearestNeighbors(10, radius=200)\n",
    "        neigh_inf.fit(nearest_cities[['longitude', 'latitude']])\n",
    "        min_dist_train, min_ind_train = neigh_inf.radius_neighbors(df_train[['longitude', 'latitude']])\n",
    "        min_dist_test, min_ind_test = neigh_inf.radius_neighbors(df_test[['longitude', 'latitude']])\n",
    "                \n",
    "        for indx, val in enumerate(indices_train):\n",
    "            answers_train[indx][0] = len(val)\n",
    "            min_dist_train[indx] = min_dist_train[indx].min()\n",
    "        for indx, val in enumerate(indices_test):\n",
    "            answers_test[indx][0] = len(val)\n",
    "            min_dist_test[indx] = min_dist_test[indx].min()\n",
    "        \n",
    "        all_answers_train.append(answers_train.astype(int))\n",
    "        all_answers_test.append(answers_test.astype(int))\n",
    "        all_districts_train = district_train\n",
    "        all_districts_test = district_test\n",
    "        all_subj_train = subject_train\n",
    "        all_subj_test = subject_test\n",
    "    \n",
    "    return all_answers_train, all_answers_test, all_districts_train, all_districts_test, all_subj_train, all_subj_test, min_dist_train, min_dist_test\n",
    "\n",
    "def get_radius_land_types_amount(df_train, df_test, land_type_df, radiuses=[1.5]):\n",
    "    all_am_train, all_am_test = [], []\n",
    "    min_dist_train, min_dist_test = np.full((df_train.shape[0], 1), 1000.0).astype(float), np.full((df_test.shape[0], 1), 1000.0).astype(float)\n",
    "    \n",
    "    neigh_inf = NearestNeighbors(1)\n",
    "    neigh_inf.fit(land_type_df[['longitude', 'latitude']])\n",
    "    min_dist_train, min_indices_train = neigh_inf.kneighbors(df_train[['longitude', 'latitude']])\n",
    "    min_dist_test, min_indices_test = neigh_inf.kneighbors(df_test[['longitude', 'latitude']])\n",
    "    \n",
    "    for radius in tqdm(radiuses):\n",
    "        am_train, am_test = np.zeros((df_train.shape[0], 1)), np.zeros((df_test.shape[0], 1))\n",
    "        \n",
    "        neigh = NearestNeighbors(10, radius=radius)\n",
    "        neigh.fit(land_type_df[['longitude', 'latitude']])\n",
    "        dist_train, indices_train = neigh.radius_neighbors(df_train[['longitude', 'latitude']])\n",
    "        dist_test, indices_test = neigh.radius_neighbors(df_test[['longitude', 'latitude']])\n",
    "                \n",
    "        for indx, val in enumerate(dist_train):\n",
    "            am_train[indx][0] = len(val)\n",
    "            min_dist_train[indx] = min_dist_train[indx].min()\n",
    "        for indx, val in enumerate(dist_test):\n",
    "            am_test[indx][0] = len(val)\n",
    "            min_dist_test[indx] = min_dist_test[indx].min()\n",
    "        \n",
    "        all_am_train.append(am_train)\n",
    "        all_am_test.append(am_test)\n",
    "    \n",
    "    return min_dist_train, min_dist_test, all_am_train, all_am_test\n",
    "\n",
    "def get_land_class_knn(df_train, df_test, land_type_df):    \n",
    "    answers_train = np.zeros((df_train.shape[0], 1))\n",
    "    answers_test = np.zeros((df_test.shape[0], 1))\n",
    "    am_train, am_test = np.zeros((df_train.shape[0], 1)), np.zeros((df_test.shape[0], 1))\n",
    "\n",
    "    neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "    neigh.fit(land_type_df[['longitude', 'latitude']], land_type_df['land_indx'])\n",
    "    land_class_train = neigh.predict(df_train[['longitude', 'latitude']])\n",
    "    land_class_test = neigh.predict(df_test[['longitude', 'latitude']])\n",
    "    \n",
    "    return land_class_train, land_class_test\n",
    "\n",
    "def get_forests_min_dist(df_train, df_test, forests_df):        \n",
    "    neigh = NearestNeighbors(1)\n",
    "    neigh.fit(forests_df[['longitude', 'latitude']])\n",
    "    answers_train, _ = neigh.kneighbors(df_train[['longitude', 'latitude']])\n",
    "    answers_test, _ = neigh.kneighbors(df_test[['longitude', 'latitude']])\n",
    "    \n",
    "    return answers_train, answers_test\n",
    "\n",
    "def get_month_radius_counters_prev(df_train, df_test, fire_types, radiuses=[1.5], only_last_year=False):\n",
    "    max_year = df_test.year.values.max()\n",
    "    min_year = df_train.year.min()\n",
    "\n",
    "    result_df_train = pd.DataFrame()\n",
    "    result_df_test = pd.DataFrame()\n",
    "    all_fire_types = []\n",
    "    for radius in radiuses:\n",
    "        fire_types_copy = fire_types.copy()\n",
    "        for indx, val in enumerate(fire_types_copy):\n",
    "            if only_last_year:\n",
    "                fire_types_copy[indx] = \"prev_years_similar_month_amount_last_year_\" + val + \"r=\" + str(radius)\n",
    "            else:\n",
    "                fire_types_copy[indx] = \"prev_years_similar_month_amount_\" + val + \"r=\" + str(radius)\n",
    "            all_fire_types.append(fire_types_copy[indx])\n",
    "        result_df_train = pd.concat([result_df_train, pd.DataFrame(np.zeros((df_train.shape[0], 11)), columns=fire_types_copy)], axis=1).reset_index(drop=True)\n",
    "        result_df_test = pd.concat([result_df_test, pd.DataFrame(np.zeros((df_test.shape[0], 11)), columns=fire_types_copy)], axis=1).reset_index(drop=True)\n",
    "\n",
    "    for rad_indx, radius in tqdm(enumerate(radiuses)):\n",
    "        for cur_year in range(min_year + 1, max_year + 1, 1):\n",
    "            for month in range(1, 13, 1):\n",
    "                for fire_indx, fire_type in enumerate(fire_types):\n",
    "                    neigh_train = NearestNeighbors(10, radius=radius)\n",
    "                    if only_last_year:\n",
    "                        mask_train_prev = (df_train.year == cur_year - 1) & (df_train.month == month) & (df_train.fire_type_name == fire_type)\n",
    "                    else:\n",
    "                        mask_train_prev = (df_train.year < cur_year) & (df_train.month == month) & (df_train.fire_type_name == fire_type)\n",
    "                    mask_train_cur = (df_train.year == cur_year) & (df_train.month == month)\n",
    "                    if df_train[mask_train_cur].shape[0] == 0 or df_train[mask_train_prev].shape[0] == 0:\n",
    "                        continue\n",
    "                    neigh_train.fit(df_train[mask_train_prev][['longitude', 'latitude']])\n",
    "                    answers, answers_indx = neigh_train.radius_neighbors(df_train[mask_train_cur][['longitude', 'latitude']])\n",
    "                    for indx, _ in enumerate(answers):\n",
    "                        answers[indx] = answers[indx].shape[0]\n",
    "                    result_df_train.loc[mask_train_cur, all_fire_types[rad_indx * 11 + fire_indx]] = answers\n",
    "        if only_last_year:\n",
    "            result_df_train['sum_events_last_year_month_in_radius=' + str(radius)] = result_df_train.sum(axis=1)\n",
    "        else:\n",
    "            result_df_train['sum_events_all_years_month_in_radius=' + str(radius)] = result_df_train.sum(axis=1)\n",
    "    \n",
    "    for rad_indx, radius in tqdm(enumerate(radiuses)):\n",
    "        for cur_year in range(min_year + 1, max_year + 1, 1):\n",
    "            for month in range(1, 13, 1):\n",
    "                for fire_indx, fire_type in enumerate(fire_types):\n",
    "                    neigh_test = NearestNeighbors(10, radius=radius)\n",
    "                    if only_last_year:\n",
    "                        mask_test_prev = (df_train.year == cur_year - 1) & (df_train.month == month) & (df_train.fire_type_name == fire_type)\n",
    "                    else:\n",
    "                        mask_test_prev = (df_train.year < cur_year) & (df_train.month == month) & (df_train.fire_type_name == fire_type)\n",
    "                    mask_test_cur = (df_test.year == cur_year) & (df_test.month == month)\n",
    "                    if df_test[mask_test_cur].shape[0] == 0 or df_test[mask_test_prev].shape[0] == 0:\n",
    "                        continue\n",
    "                    neigh_test.fit(df_test[mask_test_prev][['longitude', 'latitude']])\n",
    "                    answers, answers_indx = neigh_test.radius_neighbors(df_test[mask_test_cur][['longitude', 'latitude']])\n",
    "                    for indx, _ in enumerate(answers):\n",
    "                        answers[indx] = answers[indx].shape[0]\n",
    "                    result_df_test.loc[mask_test_cur, all_fire_types[rad_indx * 11 + fire_indx]] = answers\n",
    "        if only_last_year:\n",
    "            result_df_test['sum_events_last_year_month_in_radius=' + str(radius)] = result_df_test.sum(axis=1)\n",
    "        else:\n",
    "            result_df_test['sum_events_all_years_month_in_radius=' + str(radius)] = result_df_test.sum(axis=1)\n",
    "    \n",
    "    return result_df_train, result_df_test\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    gt = np.zeros_like(y_pred, dtype=np.int8)\n",
    "    for index, y in enumerate(y_true):\n",
    "        gt[index, y - 1] = 1\n",
    "    result = {'roc_auc_micro': roc_auc_score(gt, y_pred, average='micro')}\n",
    "    for ft in range(1, 12):\n",
    "        gt = (y_true == ft)\n",
    "        if gt.max() == gt.min():\n",
    "            roc_auc = 0\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(gt, y_pred[:, ft - 1])\n",
    "        result[f'roc_auc_{ft}'] = roc_auc\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сливаем все основные фичи в один датафрейм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_features = pd.DataFrame(columns=features_list[0].columns)\n",
    "# for item in features_list:\n",
    "#     df_features = pd.concat([df_features, item])\n",
    "# df_features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_types = ['технологический процесс', 'техногенный пожар', 'горение мусора', 'сжигание порубочных остатков',\n",
    "'сжигание мусора', 'неконтролируемый пал', 'торфяной пожар', 'лесной пожар', 'природный пожар', \n",
    "'контролируемый пал','не подтверждено']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features = df_features.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features = pd.read_csv('basic_all_ncep_features.csv') не стоит образать внимания - тут просто были записаны все вычисления,\n",
    "# чтобы каждый раз долго не экстрактить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Экстрактим самые базовые фичи. Для того, чтобы помочь катбусту понять, что 31 и 1 числа месяца близки а также 365 и 1 день в году близки будем использовать периодические функции типа cos и sin. Также покроем РФ сеткой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_features['surface_height'] = df_features.apply(lambda x: height_surf.sel(lon=x['longitude'], lat=x['latitude'],\n",
    "#                                                                             method='nearest').hgt.to_dataframe()['hgt'].values.tolist()[0], axis = 1)\n",
    "\n",
    "# df_features['land_mask'] = df_features.apply(lambda x: land.sel(lon=x['longitude'], lat=x['latitude'],\n",
    "#                                                                             method='nearest').land.to_dataframe()['land'].values.tolist()[0], axis = 1)\n",
    "\n",
    "long_min = 10\n",
    "lat_min = 40\n",
    "long_step = 10\n",
    "lat_step = 5\n",
    "\n",
    "num_long_areas = (180 - long_min) // long_step + 1\n",
    "num_lat_areas = (75 - lat_min) // lat_step + 1\n",
    "\n",
    "df_features['latitude_area'] = df_features.apply(lambda x: int((x['latitude'] - lat_min) // lat_step + 1), axis=1)\n",
    "df_features['longitude_area'] = df_features.apply(lambda x: int((x['longitude'] - long_min) // long_step + 1), axis=1)\n",
    "df_features['area_num'] = df_features.apply(lambda x: int((x['longitude_area'] - 1) * num_lat_areas + x['latitude_area']), axis=1)\n",
    "df_features['datetime'] = df_features['date'].apply(lambda x: datetime.date(int(x[:4]), int(x[5:7]), int(x[8:10])))\n",
    "df_features['day_from_start']  = df_features['datetime'].apply(lambda x: (x - date(2012, 1, 1)).days)\n",
    "df_features['day_of_year'] = df_features['datetime'].apply(lambda x: (x - date(x.year, 1, 1)).days)\n",
    "df_features['day_of_year_cos'] = df_features['day_of_year'].apply(lambda x: np.cos(2 * np.pi * x / 365))\n",
    "df_features['day_of_year_sin'] = df_features['day_of_year'].apply(lambda x: np.sin(2 * np.pi * x / 365))\n",
    "df_features['day_of_month_sin'] = df_features['day'].apply(lambda x: np.sin(2 * np.pi * x / 31))\n",
    "df_features['day_of_month_cos'] = df_features['day'].apply(lambda x: np.cos(2 * np.pi * x / 31))\n",
    "# df_features['is_winter'] = df_features.apply(lambda x: int(x['month'] in [12, 1, 2]), axis=1)\n",
    "# df_features['is_autumn'] = df_features.apply(lambda x: int(x['month'] in [9, 10, 11]), axis=1)\n",
    "# df_features['is_summer'] = df_features.apply(lambda x: int(x['month'] in [6, 7, 8]), axis=1)\n",
    "# df_features['is_spring'] = df_features.apply(lambda x: int(x['month'] in [3, 4, 5]), axis=1)\n",
    "# df_features['year'] = df_features.apply(lambda x: int(x.date[:4]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_features.to_csv('basic_all_ncep_features.csv', index=False)\n",
    "# df_features = pd.read_csv('basic_all_ncep_features.csv')\n",
    "# df_features['date'] = df_features['date'].apply(lambda x: datetime.date(int(x[:4]), int(x[5:7]), int(x[8:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим выборки на тренировочную и валидационную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = []\n",
    "n_valid_rows = 16011\n",
    "df_catboost = df_features.copy()\n",
    "df_catboost['date'] = df_catboost['date'].astype(str)\n",
    "df_catboost = df_catboost.sort_values('date')\n",
    "df_features_val = df_catboost.iloc[-n_valid_rows:].reset_index(drop=True)\n",
    "df_features_train = df_catboost.iloc[: -n_valid_rows].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features_train = df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь заэкстрактим все фичи на основе KNN для тренировочной и валидационной выборок. (Отдельно их экстрактим по причине, что на тесте нам будет доступна тетовая выборка, в без тренировочной knn не обойтись, потому удобнее выччислять отдельно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 1/2 [00:01<00:01,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 1/2 [00:01<00:01,  1.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:21, 21.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:19, 19.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 1/2 [00:34<00:34, 34.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 2/2 [01:21<00:00, 40.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:01,  1.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:02,  1.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "3it [00:03,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:05,  1.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "5it [00:06,  1.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "6it [00:07,  1.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "7it [00:08,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "8it [00:10,  1.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "9it [00:11,  1.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "10it [00:13,  1.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "11it [00:14,  1.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 1/3 [00:13<00:26, 13.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 2/3 [00:29<00:14, 14.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 3/3 [00:45<00:00, 15.05s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "land_class_train, land_class_test = get_land_class_knn(df_features_train, df_features_val, merged_land_df)\n",
    "nature_forest_min_dist_train, nature_forest_min_dist_test = get_forests_min_dist(df_features_train, df_features_val, forests_df)\n",
    "radiuses_land = [1.0, 2.5]\n",
    "min_dist_forest_train, min_dist_forest_test, am_forest_train, am_forest_test = get_radius_land_types_amount(df_features_train, df_features_val, forest_info, radiuses_land)\n",
    "min_dist_field_train, min_dist_field_test, am_field_train, am_field_test = get_radius_land_types_amount(df_features_train, df_features_val, field_info, radiuses_land)\n",
    "radiuses_event_month = [2.5]\n",
    "events_am_rad_month_all_train, events_am_rad_month_all_test = get_month_radius_counters_prev(df_features_train, df_features_val, fire_types, radiuses_event_month)\n",
    "events_am_rad_month_last_year_train, events_am_rad_month_last_year_test = get_month_radius_counters_prev(df_features_train, df_features_val, fire_types, radiuses_event_month, True)\n",
    "radiuses_event = [1.0, 1.75, 2.5]\n",
    "events_am_rad_train, events_am_rad_test, min_dist_event_train, min_dist_event_test, max_dist_event_train, max_dist_event_test = get_radius_counters(df_features_train, df_features_val, fire_types, radiuses_event)\n",
    "radiuses_city = [5, 10, 15]\n",
    "city_am_rad_train, city_am_rad_test, city_district_train, city_district_test, city_subj_train, city_subj_test, city_min_dist_train, city_min_dist_test = get_radius_cities_amount(df_features_train, df_features_val, cities_info, radiuses_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основе feature_importance были выделены фичи, которые вносят околонулевой вклад (меньше сотой доли процента и шумят в катбусте), они будут дропнуты перед началом обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "awful_features = ['humidity 100',\n",
    " 'humidity_1w_mean 100',\n",
    " 'humidity_2w_mean 100',\n",
    " 'humidity_3w_mean 100',\n",
    " 'humidity 150',\n",
    " 'humidity_1w_mean 150',\n",
    " 'humidity_2w_mean 150',\n",
    " 'humidity_3w_mean 150',\n",
    " 'trpp_1w_mean150',\n",
    " 'trpp_2w_mean150',\n",
    " 'trpt_1w_mean150',\n",
    " 'trpt_3w_mean150',\n",
    " 'humidity 200',\n",
    " 'humidity_1w_mean 200',\n",
    " 'humidity_2w_mean 200',\n",
    " 'humidity_3w_mean 200',\n",
    " 'trpp_2w_mean200',\n",
    " 'trpp_3w_mean200',\n",
    " 'trpt_1w_mean200',\n",
    " 'trpt_3w_mean200',\n",
    " 'trpp_3w_mean500',\n",
    " 'trpt_1w_mean500',\n",
    " 'trpt_3w_mean500',\n",
    " 'trpp_1w_mean1000',\n",
    " 'trpt_2w_mean1000',\n",
    " 'land_mask',\n",
    " 'is_winter',\n",
    " 'is_summer',\n",
    " 'технологический процесс_r=1.0',\n",
    " 'торфяной пожар_r=1.0',\n",
    " 'land_class',\n",
    " 'datetime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединяем всё в один большой датафрейм (очень удобно, что можно регулировать, что туда конкатить)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158860, 241)\n",
      "(16011, 241)\n"
     ]
    }
   ],
   "source": [
    "def concat_features(df_init, additional_info, col_str_list, radiuses=[], exp=False):\n",
    "    if radiuses == []:\n",
    "        if exp == True:\n",
    "            return pd.concat([df_init, pd.DataFrame(np.exp(additional_info), columns=col_str_list)], axis=1)\n",
    "        else:\n",
    "            return pd.concat([df_init, pd.DataFrame(additional_info, columns=col_str_list)], axis=1)\n",
    "    else:\n",
    "        df_add = np.array(additional_info).copy()\n",
    "        df_copy = df_init.copy()\n",
    "        for indx in range(df_add.shape[0]):\n",
    "            col_str_list_copy = col_str_list.copy()\n",
    "            for i, col in enumerate(col_str_list_copy):\n",
    "                col_str_list_copy[i] = col_str_list_copy[i] + \"_r=\" + str(radiuses[indx])\n",
    "            df_copy = pd.concat([df_copy, pd.DataFrame(df_add[indx], columns=col_str_list_copy)], axis=1)\n",
    "\n",
    "        return df_copy\n",
    "\n",
    "def concat_all_features(df_init, fire_types, events_am_rad, events_am_rad_month_df, events_am_rad_month_df_last_year, \n",
    "                        radiuses_event, radiuses_land, city_am_rad, radiuses_city,\n",
    "                        city_district, city_min_dist, city_subj, min_dist_forest, nature_forest_min_dist, \n",
    "                        am_forest, min_dist_field, am_field, land_class, min_dist_event, max_dist_event):\n",
    "\n",
    "    df_init_copy = df_init.copy()\n",
    "\n",
    "    df_init_copy = concat_features(df_init_copy, events_am_rad, fire_types, radiuses_event)\n",
    "    df_init_copy = concat_features(df_init_copy, city_am_rad, ['amount_cities'], radiuses_city)\n",
    "    df_init_copy = concat_features(df_init_copy, city_district, ['district'])\n",
    "    df_init_copy = concat_features(df_init_copy, city_min_dist, ['cities_near_min_dist'])\n",
    "    df_init_copy = concat_features(df_init_copy, city_subj, ['subject'])\n",
    "    df_init_copy = concat_features(df_init_copy, min_dist_field, ['field_near_min_dist'])\n",
    "    df_init_copy = concat_features(df_init_copy, min_dist_forest, ['forest_near_min_dist'])\n",
    "    df_init_copy = concat_features(df_init_copy, am_field, ['amount_fields'], radiuses_land)\n",
    "    df_init_copy = concat_features(df_init_copy, am_forest, ['amount_forests'], radiuses_land)\n",
    "    df_init_copy = concat_features(df_init_copy, land_class, ['land_class'])\n",
    "    df_init_copy = concat_features(df_init_copy, nature_forest_min_dist, ['nature_forest_min_dist'])\n",
    "\n",
    "    fire_types_copy = fire_types.copy()\n",
    "    for indx, val in enumerate(fire_types_copy):\n",
    "        fire_types_copy[indx] = \"max_dist_\" + val\n",
    "    \n",
    "    df_init_copy = concat_features(df_init_copy, max_dist_event, fire_types_copy)\n",
    "\n",
    "    fire_types_copy = fire_types.copy()\n",
    "    for indx, val in enumerate(fire_types_copy):\n",
    "        fire_types_copy[indx] = \"min_dist_\" + val\n",
    "\n",
    "    df_init_copy = concat_features(df_init_copy, min_dist_event, fire_types_copy)\n",
    "    \n",
    "    df_init_copy = pd.concat([df_init_copy, events_am_rad_month_df], axis=1)\n",
    "#     df_init_copy = pd.concat([df_init_copy, events_am_rad_month_df_last_year], axis=1)\n",
    "    \n",
    "    print(df_init_copy.shape)\n",
    "\n",
    "    return df_init_copy\n",
    "\n",
    "\n",
    "df_features_with_counters_train = concat_all_features(df_features_train, fire_types,\n",
    "                                                      events_am_rad_train, events_am_rad_month_all_train, events_am_rad_month_last_year_train, \n",
    "                                                      radiuses_event, radiuses_land, city_am_rad_train, radiuses_city, city_district_train,\n",
    "                                                      city_min_dist_train, city_subj_train,\n",
    "                                                      min_dist_forest_train, nature_forest_min_dist_train, am_forest_train,\n",
    "                                                      min_dist_field_train, am_field_train,\n",
    "                                                      land_class_train, min_dist_event_train, max_dist_event_train)\n",
    "\n",
    "\n",
    "df_features_with_counters_test = concat_all_features(df_features_val, fire_types,\n",
    "                                                     events_am_rad_test, events_am_rad_month_all_test, events_am_rad_month_last_year_test, \n",
    "                                                     radiuses_event, radiuses_land,city_am_rad_test, radiuses_city, city_district_test,\n",
    "                                                     city_min_dist_test, city_subj_test,\n",
    "                                                     min_dist_forest_test, nature_forest_min_dist_test, am_forest_test,\n",
    "                                                     min_dist_field_test, am_field_test,\n",
    "                                                     land_class_test, min_dist_event_test, max_dist_event_test)\n",
    "\n",
    "# df_features_with_counters_train, df_features_with_counters_test = get_count_subj_month(df_features_with_counters_train, df_features_with_counters_test, 'area_num')\n",
    "# df_features_with_counters_train, df_features_with_counters_test = get_count_subj_month(df_features_with_counters_train, df_features_with_counters_test, 'subject')\n",
    "# df_features_with_counters_train, df_features_with_counters_test = get_count_subj_month(df_features_with_counters_train, df_features_with_counters_test, 'district')\n",
    "# df_features_with_counters_train, df_features_with_counters_test = get_count_subj_month_only_prev(df_features_with_counters_train, df_features_with_counters_test, 'district')\n",
    "# df_features_with_counters_train, df_features_with_counters_test = get_count_subj_month_only_prev(df_features_with_counters_train, df_features_with_counters_test, 'subject')\n",
    "# df_features_with_counters_train, df_features_with_counters_test = get_count_subj_month_only_prev(df_features_with_counters_train, df_features_with_counters_test, 'area_num')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простой чекпоинт для сохранения промежуточных результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features_with_counters_train.to_csv('df_features_with_counters_train.csv', index=False)\n",
    "# df_features_with_counters_test.to_csv('df_features_with_counters_test.csv', index=False)\n",
    "# df_features_with_counters_train = pd.read_csv('df_features_with_counters_train.csv')\n",
    "# df_features_with_counters_test = pd.read_csv('df_features_with_counters_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чистим данные для тренировки и валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features=['fire_type', 'date', 'fire_type_name', 'latitude_area',\n",
    "               'longitude_area'] + awful_features\n",
    "cols = list(df_features_with_counters_test.drop(drop_features, axis=1).columns)\n",
    "\n",
    "desired_cat_features = ['month', 'district', 'subject', 'weekday']\n",
    "cat_features = []\n",
    "for feature in desired_cat_features:\n",
    "    cat_features.append(cols.index(feature))\n",
    "    \n",
    "X_train = df_features_with_counters_train.drop(drop_features, axis=1)\n",
    "Y_train = df_features_with_counters_train.fire_type\n",
    "X_val = df_features_with_counters_test.drop(drop_features, axis=1)\n",
    "Y_val = df_features_with_counters_test.fire_type\n",
    "\n",
    "true_val = df_features_with_counters_test.fire_type.tolist()\n",
    "X_train_all = pd.concat([X_train, X_val]).reset_index(drop=True)\n",
    "Y_train_all = pd.concat([Y_train, Y_val]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной закомменченной части кода был опробован стакинг моделей (мб у кого-то он лучше зажжёт)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn import model_selection\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB \n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "# n_jobs = -1\n",
    "\n",
    "# clf1 = RandomForestClassifier(n_estimators=100, verbose=True, n_jobs=n_jobs)\n",
    "# clf2 = KNeighborsClassifier()\n",
    "\n",
    "# lr = catboost.CatBoostClassifier(loss_function='MultiClass',\n",
    "#                                   verbose=1000, iterations=1500, task_type='CPU', cat_features=cat_features)\n",
    "\n",
    "# sclf = StackingCVClassifier(classifiers=[clf1, clf2], \n",
    "#                           meta_classifier=lr, use_features_in_secondary=True, verbose=1, n_jobs=n_jobs, cv=50)\n",
    "\n",
    "# sclf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# drop_susp_feat = ['events_amount_passed_in_area_num', 'events_amount_passed_in_subject', \n",
    "#                   'events_amount_passed_in_district']\n",
    "# drop_susp_feat = []\n",
    "# df_features_with_counters_train.drop(columns=drop_susp_feat).to_csv('./solution/df_features_train.csv',\n",
    "#                                                                     index=False)\n",
    "# \n",
    "# with open('solution/model.pickle', 'wb') as fout:\n",
    "#     pickle.dump(sclf, fout, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренируем катбуст и дополнительно выводим трениовочные и валидационные метрики + confusion matrix. Анализируем результаты, докидываем идеи..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n",
      "0:\tlearn: 2.3161215\ttest: 2.3172767\tbest: 2.3172767 (0)\ttotal: 651ms\tremaining: 1h 48m 27s\n",
      "10:\tlearn: 1.8578534\ttest: 1.8668354\tbest: 1.8668354 (10)\ttotal: 6.19s\tremaining: 1h 33m 40s\n",
      "20:\tlearn: 1.6371092\ttest: 1.6488593\tbest: 1.6488593 (20)\ttotal: 11.8s\tremaining: 1h 33m 10s\n",
      "30:\tlearn: 1.5013154\ttest: 1.5200859\tbest: 1.5200859 (30)\ttotal: 17.2s\tremaining: 1h 32m 24s\n",
      "40:\tlearn: 1.4074460\ttest: 1.4403943\tbest: 1.4403943 (40)\ttotal: 22.7s\tremaining: 1h 31m 57s\n",
      "50:\tlearn: 1.3404872\ttest: 1.3848741\tbest: 1.3848741 (50)\ttotal: 28.2s\tremaining: 1h 31m 37s\n",
      "60:\tlearn: 1.2902764\ttest: 1.3459033\tbest: 1.3459033 (60)\ttotal: 33.6s\tremaining: 1h 31m 12s\n",
      "70:\tlearn: 1.2521702\ttest: 1.3192719\tbest: 1.3192719 (70)\ttotal: 38.9s\tremaining: 1h 30m 45s\n",
      "80:\tlearn: 1.2224874\ttest: 1.2989529\tbest: 1.2989529 (80)\ttotal: 44.3s\tremaining: 1h 30m 29s\n",
      "90:\tlearn: 1.1969524\ttest: 1.2853104\tbest: 1.2853104 (90)\ttotal: 49.9s\tremaining: 1h 30m 28s\n",
      "100:\tlearn: 1.1768003\ttest: 1.2741444\tbest: 1.2741444 (100)\ttotal: 55.2s\tremaining: 1h 30m 13s\n",
      "110:\tlearn: 1.1590414\ttest: 1.2673164\tbest: 1.2673164 (110)\ttotal: 1m\tremaining: 1h 30m 7s\n",
      "120:\tlearn: 1.1451531\ttest: 1.2598147\tbest: 1.2598147 (120)\ttotal: 1m 6s\tremaining: 1h 29m 58s\n",
      "130:\tlearn: 1.1323192\ttest: 1.2530469\tbest: 1.2530469 (130)\ttotal: 1m 11s\tremaining: 1h 29m 49s\n",
      "140:\tlearn: 1.1218868\ttest: 1.2479433\tbest: 1.2479433 (140)\ttotal: 1m 16s\tremaining: 1h 29m 23s\n",
      "150:\tlearn: 1.1104926\ttest: 1.2428247\tbest: 1.2428247 (150)\ttotal: 1m 21s\tremaining: 1h 29m\n",
      "160:\tlearn: 1.1003203\ttest: 1.2394172\tbest: 1.2394165 (159)\ttotal: 1m 26s\tremaining: 1h 28m 33s\n",
      "170:\tlearn: 1.0922303\ttest: 1.2365716\tbest: 1.2365716 (170)\ttotal: 1m 32s\tremaining: 1h 28m 24s\n",
      "180:\tlearn: 1.0852499\ttest: 1.2335200\tbest: 1.2335200 (180)\ttotal: 1m 37s\tremaining: 1h 28m 10s\n",
      "190:\tlearn: 1.0781623\ttest: 1.2310570\tbest: 1.2310570 (190)\ttotal: 1m 42s\tremaining: 1h 28m 1s\n",
      "200:\tlearn: 1.0712002\ttest: 1.2296609\tbest: 1.2296143 (196)\ttotal: 1m 48s\tremaining: 1h 28m\n",
      "210:\tlearn: 1.0646467\ttest: 1.2283738\tbest: 1.2283738 (210)\ttotal: 1m 53s\tremaining: 1h 27m 56s\n",
      "220:\tlearn: 1.0582613\ttest: 1.2259689\tbest: 1.2259689 (220)\ttotal: 1m 59s\tremaining: 1h 27m 51s\n",
      "230:\tlearn: 1.0523789\ttest: 1.2255371\tbest: 1.2253536 (229)\ttotal: 2m 4s\tremaining: 1h 27m 43s\n",
      "240:\tlearn: 1.0471607\ttest: 1.2234207\tbest: 1.2234207 (240)\ttotal: 2m 9s\tremaining: 1h 27m 35s\n",
      "250:\tlearn: 1.0421408\ttest: 1.2221516\tbest: 1.2221516 (250)\ttotal: 2m 15s\tremaining: 1h 27m 34s\n",
      "260:\tlearn: 1.0372375\ttest: 1.2225608\tbest: 1.2221516 (250)\ttotal: 2m 20s\tremaining: 1h 27m 25s\n",
      "270:\tlearn: 1.0327239\ttest: 1.2209302\tbest: 1.2209302 (270)\ttotal: 2m 25s\tremaining: 1h 27m 15s\n",
      "280:\tlearn: 1.0285547\ttest: 1.2211931\tbest: 1.2207061 (273)\ttotal: 2m 31s\tremaining: 1h 27m 11s\n",
      "290:\tlearn: 1.0245992\ttest: 1.2204193\tbest: 1.2203623 (287)\ttotal: 2m 36s\tremaining: 1h 27m 6s\n",
      "300:\tlearn: 1.0202465\ttest: 1.2193321\tbest: 1.2188615 (298)\ttotal: 2m 42s\tremaining: 1h 27m 1s\n",
      "310:\tlearn: 1.0167965\ttest: 1.2173936\tbest: 1.2173011 (309)\ttotal: 2m 47s\tremaining: 1h 26m 51s\n",
      "320:\tlearn: 1.0127169\ttest: 1.2160136\tbest: 1.2160136 (320)\ttotal: 2m 52s\tremaining: 1h 26m 41s\n",
      "330:\tlearn: 1.0093680\ttest: 1.2150172\tbest: 1.2150002 (329)\ttotal: 2m 57s\tremaining: 1h 26m 31s\n",
      "340:\tlearn: 1.0062421\ttest: 1.2147120\tbest: 1.2143442 (338)\ttotal: 3m 3s\tremaining: 1h 26m 27s\n",
      "350:\tlearn: 1.0030319\ttest: 1.2148174\tbest: 1.2142454 (345)\ttotal: 3m 8s\tremaining: 1h 26m 24s\n",
      "360:\tlearn: 0.9999172\ttest: 1.2145531\tbest: 1.2142454 (345)\ttotal: 3m 13s\tremaining: 1h 26m 18s\n",
      "370:\tlearn: 0.9965353\ttest: 1.2131961\tbest: 1.2131961 (370)\ttotal: 3m 19s\tremaining: 1h 26m 16s\n",
      "380:\tlearn: 0.9937503\ttest: 1.2126889\tbest: 1.2126179 (379)\ttotal: 3m 24s\tremaining: 1h 26m 6s\n",
      "390:\tlearn: 0.9909214\ttest: 1.2116517\tbest: 1.2116517 (390)\ttotal: 3m 29s\tremaining: 1h 25m 56s\n",
      "400:\tlearn: 0.9880015\ttest: 1.2130103\tbest: 1.2116517 (390)\ttotal: 3m 35s\tremaining: 1h 25m 52s\n",
      "410:\tlearn: 0.9851472\ttest: 1.2120814\tbest: 1.2116517 (390)\ttotal: 3m 40s\tremaining: 1h 25m 46s\n",
      "420:\tlearn: 0.9823640\ttest: 1.2113116\tbest: 1.2111800 (418)\ttotal: 3m 45s\tremaining: 1h 25m 39s\n",
      "430:\tlearn: 0.9795287\ttest: 1.2114768\tbest: 1.2111370 (423)\ttotal: 3m 51s\tremaining: 1h 25m 36s\n",
      "440:\tlearn: 0.9770374\ttest: 1.2115377\tbest: 1.2111124 (433)\ttotal: 3m 56s\tremaining: 1h 25m 31s\n",
      "450:\tlearn: 0.9744497\ttest: 1.2119615\tbest: 1.2111124 (433)\ttotal: 4m 2s\tremaining: 1h 25m 24s\n",
      "460:\tlearn: 0.9720871\ttest: 1.2114735\tbest: 1.2111124 (433)\ttotal: 4m 7s\tremaining: 1h 25m 19s\n",
      "470:\tlearn: 0.9696459\ttest: 1.2107913\tbest: 1.2107913 (470)\ttotal: 4m 12s\tremaining: 1h 25m 10s\n",
      "480:\tlearn: 0.9670874\ttest: 1.2108465\tbest: 1.2104400 (473)\ttotal: 4m 18s\tremaining: 1h 25m 7s\n",
      "490:\tlearn: 0.9649560\ttest: 1.2107137\tbest: 1.2104400 (473)\ttotal: 4m 23s\tremaining: 1h 25m 1s\n",
      "500:\tlearn: 0.9623008\ttest: 1.2097402\tbest: 1.2096706 (498)\ttotal: 4m 28s\tremaining: 1h 24m 53s\n",
      "510:\tlearn: 0.9600748\ttest: 1.2093957\tbest: 1.2093427 (509)\ttotal: 4m 34s\tremaining: 1h 24m 49s\n",
      "520:\tlearn: 0.9574384\ttest: 1.2085999\tbest: 1.2085999 (520)\ttotal: 4m 39s\tremaining: 1h 24m 42s\n",
      "530:\tlearn: 0.9553695\ttest: 1.2088833\tbest: 1.2084850 (521)\ttotal: 4m 44s\tremaining: 1h 24m 37s\n",
      "540:\tlearn: 0.9533005\ttest: 1.2091948\tbest: 1.2084850 (521)\ttotal: 4m 50s\tremaining: 1h 24m 32s\n",
      "550:\tlearn: 0.9510592\ttest: 1.2080652\tbest: 1.2080652 (550)\ttotal: 4m 55s\tremaining: 1h 24m 25s\n",
      "560:\tlearn: 0.9491225\ttest: 1.2074476\tbest: 1.2074169 (559)\ttotal: 5m\tremaining: 1h 24m 21s\n",
      "570:\tlearn: 0.9471068\ttest: 1.2063070\tbest: 1.2063070 (570)\ttotal: 5m 6s\tremaining: 1h 24m 16s\n",
      "580:\tlearn: 0.9451928\ttest: 1.2057445\tbest: 1.2056745 (578)\ttotal: 5m 11s\tremaining: 1h 24m 10s\n",
      "590:\tlearn: 0.9432006\ttest: 1.2053510\tbest: 1.2053510 (590)\ttotal: 5m 16s\tremaining: 1h 24m 2s\n",
      "600:\tlearn: 0.9411789\ttest: 1.2062939\tbest: 1.2053510 (590)\ttotal: 5m 21s\tremaining: 1h 23m 54s\n",
      "610:\tlearn: 0.9391812\ttest: 1.2061876\tbest: 1.2053510 (590)\ttotal: 5m 27s\tremaining: 1h 23m 46s\n",
      "620:\tlearn: 0.9372819\ttest: 1.2055846\tbest: 1.2053510 (590)\ttotal: 5m 32s\tremaining: 1h 23m 38s\n",
      "630:\tlearn: 0.9354436\ttest: 1.2050990\tbest: 1.2050353 (625)\ttotal: 5m 37s\tremaining: 1h 23m 30s\n",
      "640:\tlearn: 0.9337959\ttest: 1.2061829\tbest: 1.2050353 (625)\ttotal: 5m 42s\tremaining: 1h 23m 24s\n",
      "650:\tlearn: 0.9318709\ttest: 1.2058824\tbest: 1.2050353 (625)\ttotal: 5m 48s\tremaining: 1h 23m 20s\n",
      "660:\tlearn: 0.9301757\ttest: 1.2061885\tbest: 1.2050353 (625)\ttotal: 5m 53s\tremaining: 1h 23m 13s\n",
      "670:\tlearn: 0.9286662\ttest: 1.2054170\tbest: 1.2050353 (625)\ttotal: 5m 58s\tremaining: 1h 23m 7s\n",
      "680:\tlearn: 0.9269406\ttest: 1.2052345\tbest: 1.2050353 (625)\ttotal: 6m 3s\tremaining: 1h 22m 59s\n",
      "690:\tlearn: 0.9251203\ttest: 1.2052886\tbest: 1.2047142 (686)\ttotal: 6m 9s\tremaining: 1h 22m 56s\n",
      "700:\tlearn: 0.9237020\ttest: 1.2054719\tbest: 1.2047142 (686)\ttotal: 6m 14s\tremaining: 1h 22m 52s\n",
      "710:\tlearn: 0.9220090\ttest: 1.2048864\tbest: 1.2047142 (686)\ttotal: 6m 20s\tremaining: 1h 22m 47s\n",
      "720:\tlearn: 0.9201794\ttest: 1.2045659\tbest: 1.2045659 (720)\ttotal: 6m 25s\tremaining: 1h 22m 40s\n",
      "730:\tlearn: 0.9187838\ttest: 1.2041596\tbest: 1.2041222 (728)\ttotal: 6m 30s\tremaining: 1h 22m 35s\n",
      "740:\tlearn: 0.9173512\ttest: 1.2042009\tbest: 1.2040456 (732)\ttotal: 6m 36s\tremaining: 1h 22m 29s\n",
      "750:\tlearn: 0.9159795\ttest: 1.2040723\tbest: 1.2037977 (745)\ttotal: 6m 41s\tremaining: 1h 22m 22s\n",
      "760:\tlearn: 0.9147475\ttest: 1.2042168\tbest: 1.2037977 (745)\ttotal: 6m 46s\tremaining: 1h 22m 16s\n",
      "770:\tlearn: 0.9133520\ttest: 1.2042567\tbest: 1.2037977 (745)\ttotal: 6m 51s\tremaining: 1h 22m 11s\n",
      "780:\tlearn: 0.9116909\ttest: 1.2035700\tbest: 1.2035700 (780)\ttotal: 6m 57s\tremaining: 1h 22m 6s\n",
      "790:\tlearn: 0.9103326\ttest: 1.2033115\tbest: 1.2033115 (790)\ttotal: 7m 2s\tremaining: 1h 22m 1s\n",
      "800:\tlearn: 0.9090584\ttest: 1.2029012\tbest: 1.2029012 (800)\ttotal: 7m 8s\tremaining: 1h 21m 56s\n",
      "810:\tlearn: 0.9078018\ttest: 1.2021873\tbest: 1.2020672 (808)\ttotal: 7m 13s\tremaining: 1h 21m 51s\n",
      "820:\tlearn: 0.9063812\ttest: 1.2023146\tbest: 1.2020090 (814)\ttotal: 7m 18s\tremaining: 1h 21m 47s\n",
      "830:\tlearn: 0.9047100\ttest: 1.2021868\tbest: 1.2020090 (814)\ttotal: 7m 24s\tremaining: 1h 21m 41s\n",
      "840:\tlearn: 0.9034278\ttest: 1.2016505\tbest: 1.2016505 (840)\ttotal: 7m 29s\tremaining: 1h 21m 35s\n",
      "850:\tlearn: 0.9022223\ttest: 1.2014554\tbest: 1.2014554 (850)\ttotal: 7m 34s\tremaining: 1h 21m 29s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "860:\tlearn: 0.9010286\ttest: 1.2015726\tbest: 1.2013292 (851)\ttotal: 7m 40s\tremaining: 1h 21m 22s\n",
      "870:\tlearn: 0.8997553\ttest: 1.2016264\tbest: 1.2013292 (851)\ttotal: 7m 45s\tremaining: 1h 21m 15s\n",
      "880:\tlearn: 0.8987355\ttest: 1.2011874\tbest: 1.2011874 (880)\ttotal: 7m 50s\tremaining: 1h 21m 9s\n",
      "890:\tlearn: 0.8976672\ttest: 1.2000505\tbest: 1.2000505 (890)\ttotal: 7m 55s\tremaining: 1h 21m 4s\n",
      "900:\tlearn: 0.8964863\ttest: 1.2000171\tbest: 1.2000171 (900)\ttotal: 8m 1s\tremaining: 1h 20m 58s\n",
      "910:\tlearn: 0.8952676\ttest: 1.2000835\tbest: 1.1999846 (902)\ttotal: 8m 6s\tremaining: 1h 20m 53s\n",
      "920:\tlearn: 0.8940597\ttest: 1.1992095\tbest: 1.1991205 (919)\ttotal: 8m 11s\tremaining: 1h 20m 47s\n",
      "930:\tlearn: 0.8928172\ttest: 1.1993329\tbest: 1.1990655 (923)\ttotal: 8m 16s\tremaining: 1h 20m 40s\n",
      "940:\tlearn: 0.8918616\ttest: 1.1981984\tbest: 1.1981546 (939)\ttotal: 8m 22s\tremaining: 1h 20m 34s\n",
      "950:\tlearn: 0.8905365\ttest: 1.1982456\tbest: 1.1981031 (943)\ttotal: 8m 27s\tremaining: 1h 20m 29s\n",
      "960:\tlearn: 0.8894015\ttest: 1.1980255\tbest: 1.1978597 (956)\ttotal: 8m 32s\tremaining: 1h 20m 23s\n",
      "970:\tlearn: 0.8881050\ttest: 1.1983872\tbest: 1.1978597 (956)\ttotal: 8m 38s\tremaining: 1h 20m 17s\n",
      "980:\tlearn: 0.8868273\ttest: 1.1983745\tbest: 1.1978597 (956)\ttotal: 8m 43s\tremaining: 1h 20m 13s\n",
      "990:\tlearn: 0.8856459\ttest: 1.1981119\tbest: 1.1978597 (956)\ttotal: 8m 48s\tremaining: 1h 20m 8s\n",
      "1000:\tlearn: 0.8846017\ttest: 1.1981352\tbest: 1.1978597 (956)\ttotal: 8m 54s\tremaining: 1h 20m 2s\n",
      "1010:\tlearn: 0.8833389\ttest: 1.1982553\tbest: 1.1978597 (956)\ttotal: 8m 59s\tremaining: 1h 19m 57s\n",
      "1020:\tlearn: 0.8821235\ttest: 1.1979921\tbest: 1.1978597 (956)\ttotal: 9m 5s\tremaining: 1h 19m 53s\n",
      "1030:\tlearn: 0.8812269\ttest: 1.1976618\tbest: 1.1976618 (1030)\ttotal: 9m 10s\tremaining: 1h 19m 48s\n",
      "1040:\tlearn: 0.8799815\ttest: 1.1972480\tbest: 1.1972480 (1040)\ttotal: 9m 15s\tremaining: 1h 19m 44s\n",
      "1050:\tlearn: 0.8788552\ttest: 1.1970104\tbest: 1.1970104 (1050)\ttotal: 9m 21s\tremaining: 1h 19m 40s\n",
      "1060:\tlearn: 0.8778614\ttest: 1.1971868\tbest: 1.1969688 (1051)\ttotal: 9m 26s\tremaining: 1h 19m 35s\n",
      "1070:\tlearn: 0.8769130\ttest: 1.1970036\tbest: 1.1969688 (1051)\ttotal: 9m 32s\tremaining: 1h 19m 30s\n",
      "1080:\tlearn: 0.8758666\ttest: 1.1963566\tbest: 1.1963566 (1080)\ttotal: 9m 37s\tremaining: 1h 19m 26s\n",
      "1090:\tlearn: 0.8749377\ttest: 1.1959490\tbest: 1.1959441 (1089)\ttotal: 9m 43s\tremaining: 1h 19m 21s\n",
      "1100:\tlearn: 0.8739434\ttest: 1.1960685\tbest: 1.1959441 (1089)\ttotal: 9m 48s\tremaining: 1h 19m 17s\n",
      "1110:\tlearn: 0.8730233\ttest: 1.1961129\tbest: 1.1959441 (1089)\ttotal: 9m 54s\tremaining: 1h 19m 13s\n",
      "1120:\tlearn: 0.8720101\ttest: 1.1961632\tbest: 1.1959441 (1089)\ttotal: 9m 59s\tremaining: 1h 19m 8s\n",
      "1130:\tlearn: 0.8710492\ttest: 1.1958734\tbest: 1.1957768 (1129)\ttotal: 10m 5s\tremaining: 1h 19m 5s\n",
      "1140:\tlearn: 0.8700763\ttest: 1.1968941\tbest: 1.1957768 (1129)\ttotal: 10m 10s\tremaining: 1h 19m 1s\n",
      "1150:\tlearn: 0.8691445\ttest: 1.1961564\tbest: 1.1957768 (1129)\ttotal: 10m 16s\tremaining: 1h 18m 57s\n",
      "1160:\tlearn: 0.8680286\ttest: 1.1951954\tbest: 1.1951954 (1160)\ttotal: 10m 21s\tremaining: 1h 18m 52s\n",
      "1170:\tlearn: 0.8671282\ttest: 1.1949141\tbest: 1.1949141 (1170)\ttotal: 10m 27s\tremaining: 1h 18m 48s\n",
      "1180:\tlearn: 0.8663441\ttest: 1.1945395\tbest: 1.1944808 (1174)\ttotal: 10m 32s\tremaining: 1h 18m 43s\n",
      "1190:\tlearn: 0.8653170\ttest: 1.1945800\tbest: 1.1943347 (1189)\ttotal: 10m 38s\tremaining: 1h 18m 39s\n",
      "1200:\tlearn: 0.8643557\ttest: 1.1944034\tbest: 1.1943347 (1189)\ttotal: 10m 43s\tremaining: 1h 18m 35s\n",
      "1210:\tlearn: 0.8635052\ttest: 1.1940410\tbest: 1.1940410 (1210)\ttotal: 10m 49s\tremaining: 1h 18m 32s\n",
      "1220:\tlearn: 0.8626106\ttest: 1.1940291\tbest: 1.1938438 (1214)\ttotal: 10m 54s\tremaining: 1h 18m 26s\n",
      "1230:\tlearn: 0.8618466\ttest: 1.1939746\tbest: 1.1938438 (1214)\ttotal: 11m\tremaining: 1h 18m 21s\n",
      "1240:\tlearn: 0.8610351\ttest: 1.1938688\tbest: 1.1937443 (1237)\ttotal: 11m 5s\tremaining: 1h 18m 17s\n",
      "1250:\tlearn: 0.8602179\ttest: 1.1937744\tbest: 1.1937386 (1245)\ttotal: 11m 11s\tremaining: 1h 18m 12s\n",
      "1260:\tlearn: 0.8594106\ttest: 1.1938988\tbest: 1.1936655 (1254)\ttotal: 11m 16s\tremaining: 1h 18m 8s\n",
      "1270:\tlearn: 0.8585876\ttest: 1.1931668\tbest: 1.1931668 (1270)\ttotal: 11m 21s\tremaining: 1h 18m 3s\n",
      "1280:\tlearn: 0.8576795\ttest: 1.1928139\tbest: 1.1928139 (1280)\ttotal: 11m 27s\tremaining: 1h 18m\n",
      "1290:\tlearn: 0.8570190\ttest: 1.1926908\tbest: 1.1926908 (1290)\ttotal: 11m 33s\tremaining: 1h 17m 55s\n",
      "1300:\tlearn: 0.8562151\ttest: 1.1932107\tbest: 1.1926908 (1290)\ttotal: 11m 38s\tremaining: 1h 17m 50s\n",
      "1310:\tlearn: 0.8553673\ttest: 1.1932564\tbest: 1.1926908 (1290)\ttotal: 11m 43s\tremaining: 1h 17m 45s\n",
      "1320:\tlearn: 0.8544159\ttest: 1.1926314\tbest: 1.1925948 (1319)\ttotal: 11m 49s\tremaining: 1h 17m 40s\n",
      "1330:\tlearn: 0.8536562\ttest: 1.1924048\tbest: 1.1924048 (1330)\ttotal: 11m 54s\tremaining: 1h 17m 35s\n",
      "1340:\tlearn: 0.8527730\ttest: 1.1929002\tbest: 1.1924048 (1330)\ttotal: 12m\tremaining: 1h 17m 29s\n",
      "1350:\tlearn: 0.8519388\ttest: 1.1929837\tbest: 1.1924048 (1330)\ttotal: 12m 5s\tremaining: 1h 17m 23s\n",
      "1360:\tlearn: 0.8512227\ttest: 1.1938723\tbest: 1.1924048 (1330)\ttotal: 12m 10s\tremaining: 1h 17m 16s\n",
      "1370:\tlearn: 0.8503386\ttest: 1.1936343\tbest: 1.1924048 (1330)\ttotal: 12m 15s\tremaining: 1h 17m 10s\n",
      "1380:\tlearn: 0.8495410\ttest: 1.1936258\tbest: 1.1924048 (1330)\ttotal: 12m 21s\tremaining: 1h 17m 5s\n",
      "1390:\tlearn: 0.8488003\ttest: 1.1937623\tbest: 1.1924048 (1330)\ttotal: 12m 26s\tremaining: 1h 17m\n",
      "1400:\tlearn: 0.8480180\ttest: 1.1938105\tbest: 1.1924048 (1330)\ttotal: 12m 31s\tremaining: 1h 16m 54s\n",
      "1410:\tlearn: 0.8472890\ttest: 1.1940090\tbest: 1.1924048 (1330)\ttotal: 12m 37s\tremaining: 1h 16m 49s\n",
      "1420:\tlearn: 0.8466228\ttest: 1.1940100\tbest: 1.1924048 (1330)\ttotal: 12m 42s\tremaining: 1h 16m 43s\n",
      "1430:\tlearn: 0.8460367\ttest: 1.1936421\tbest: 1.1924048 (1330)\ttotal: 12m 47s\tremaining: 1h 16m 37s\n",
      "1440:\tlearn: 0.8452661\ttest: 1.1934080\tbest: 1.1924048 (1330)\ttotal: 12m 53s\tremaining: 1h 16m 31s\n",
      "1450:\tlearn: 0.8443339\ttest: 1.1929653\tbest: 1.1924048 (1330)\ttotal: 12m 58s\tremaining: 1h 16m 26s\n",
      "1460:\tlearn: 0.8434868\ttest: 1.1926123\tbest: 1.1923899 (1457)\ttotal: 13m 3s\tremaining: 1h 16m 21s\n",
      "1470:\tlearn: 0.8427389\ttest: 1.1927183\tbest: 1.1923899 (1457)\ttotal: 13m 9s\tremaining: 1h 16m 16s\n",
      "1480:\tlearn: 0.8420055\ttest: 1.1925501\tbest: 1.1923899 (1457)\ttotal: 13m 14s\tremaining: 1h 16m 9s\n",
      "1490:\tlearn: 0.8411972\ttest: 1.1925141\tbest: 1.1923899 (1457)\ttotal: 13m 19s\tremaining: 1h 16m 3s\n",
      "1500:\tlearn: 0.8404131\ttest: 1.1925501\tbest: 1.1923899 (1457)\ttotal: 13m 24s\tremaining: 1h 15m 57s\n",
      "1510:\tlearn: 0.8396034\ttest: 1.1923243\tbest: 1.1923243 (1510)\ttotal: 13m 30s\tremaining: 1h 15m 52s\n",
      "1520:\tlearn: 0.8388780\ttest: 1.1924483\tbest: 1.1922109 (1517)\ttotal: 13m 35s\tremaining: 1h 15m 46s\n",
      "1530:\tlearn: 0.8381404\ttest: 1.1922117\tbest: 1.1921859 (1529)\ttotal: 13m 40s\tremaining: 1h 15m 40s\n",
      "1540:\tlearn: 0.8374178\ttest: 1.1926112\tbest: 1.1921859 (1529)\ttotal: 13m 46s\tremaining: 1h 15m 35s\n",
      "1550:\tlearn: 0.8367915\ttest: 1.1928012\tbest: 1.1921859 (1529)\ttotal: 13m 51s\tremaining: 1h 15m 29s\n",
      "1560:\tlearn: 0.8360255\ttest: 1.1930297\tbest: 1.1921859 (1529)\ttotal: 13m 56s\tremaining: 1h 15m 23s\n",
      "1570:\tlearn: 0.8353160\ttest: 1.1929158\tbest: 1.1921859 (1529)\ttotal: 14m 1s\tremaining: 1h 15m 17s\n",
      "1580:\tlearn: 0.8345754\ttest: 1.1929774\tbest: 1.1921859 (1529)\ttotal: 14m 7s\tremaining: 1h 15m 11s\n",
      "1590:\tlearn: 0.8339244\ttest: 1.1927985\tbest: 1.1921859 (1529)\ttotal: 14m 12s\tremaining: 1h 15m 5s\n",
      "1600:\tlearn: 0.8331386\ttest: 1.1924832\tbest: 1.1921859 (1529)\ttotal: 14m 17s\tremaining: 1h 14m 59s\n",
      "1610:\tlearn: 0.8325394\ttest: 1.1930838\tbest: 1.1921859 (1529)\ttotal: 14m 22s\tremaining: 1h 14m 53s\n",
      "1620:\tlearn: 0.8318456\ttest: 1.1928065\tbest: 1.1921859 (1529)\ttotal: 14m 27s\tremaining: 1h 14m 46s\n",
      "1630:\tlearn: 0.8312038\ttest: 1.1928780\tbest: 1.1921859 (1529)\ttotal: 14m 33s\tremaining: 1h 14m 40s\n",
      "1640:\tlearn: 0.8304891\ttest: 1.1923108\tbest: 1.1921859 (1529)\ttotal: 14m 38s\tremaining: 1h 14m 33s\n",
      "1650:\tlearn: 0.8299405\ttest: 1.1923001\tbest: 1.1921859 (1529)\ttotal: 14m 43s\tremaining: 1h 14m 27s\n",
      "1660:\tlearn: 0.8292970\ttest: 1.1921250\tbest: 1.1920305 (1654)\ttotal: 14m 48s\tremaining: 1h 14m 21s\n",
      "1670:\tlearn: 0.8286421\ttest: 1.1919128\tbest: 1.1918963 (1664)\ttotal: 14m 53s\tremaining: 1h 14m 15s\n",
      "1680:\tlearn: 0.8280204\ttest: 1.1919153\tbest: 1.1918578 (1674)\ttotal: 14m 59s\tremaining: 1h 14m 9s\n",
      "1690:\tlearn: 0.8274197\ttest: 1.1922179\tbest: 1.1918427 (1685)\ttotal: 15m 4s\tremaining: 1h 14m 3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700:\tlearn: 0.8267926\ttest: 1.1920157\tbest: 1.1918427 (1685)\ttotal: 15m 9s\tremaining: 1h 13m 58s\n",
      "1710:\tlearn: 0.8261523\ttest: 1.1925077\tbest: 1.1918427 (1685)\ttotal: 15m 15s\tremaining: 1h 13m 53s\n",
      "1720:\tlearn: 0.8254644\ttest: 1.1924408\tbest: 1.1918427 (1685)\ttotal: 15m 20s\tremaining: 1h 13m 47s\n",
      "1730:\tlearn: 0.8248064\ttest: 1.1925076\tbest: 1.1918427 (1685)\ttotal: 15m 25s\tremaining: 1h 13m 42s\n",
      "1740:\tlearn: 0.8242338\ttest: 1.1925400\tbest: 1.1918427 (1685)\ttotal: 15m 31s\tremaining: 1h 13m 37s\n",
      "1750:\tlearn: 0.8236280\ttest: 1.1924511\tbest: 1.1918427 (1685)\ttotal: 15m 36s\tremaining: 1h 13m 32s\n",
      "1760:\tlearn: 0.8230084\ttest: 1.1925447\tbest: 1.1918427 (1685)\ttotal: 15m 42s\tremaining: 1h 13m 27s\n",
      "1770:\tlearn: 0.8225049\ttest: 1.1924817\tbest: 1.1918427 (1685)\ttotal: 15m 47s\tremaining: 1h 13m 21s\n",
      "1780:\tlearn: 0.8217834\ttest: 1.1925272\tbest: 1.1918427 (1685)\ttotal: 15m 52s\tremaining: 1h 13m 16s\n",
      "1790:\tlearn: 0.8211668\ttest: 1.1925949\tbest: 1.1918427 (1685)\ttotal: 15m 57s\tremaining: 1h 13m 10s\n",
      "1800:\tlearn: 0.8205760\ttest: 1.1927352\tbest: 1.1918427 (1685)\ttotal: 16m 3s\tremaining: 1h 13m 4s\n",
      "1810:\tlearn: 0.8200459\ttest: 1.1924615\tbest: 1.1918427 (1685)\ttotal: 16m 8s\tremaining: 1h 12m 59s\n",
      "1820:\tlearn: 0.8193092\ttest: 1.1927444\tbest: 1.1918427 (1685)\ttotal: 16m 13s\tremaining: 1h 12m 53s\n",
      "1830:\tlearn: 0.8187756\ttest: 1.1926385\tbest: 1.1918427 (1685)\ttotal: 16m 18s\tremaining: 1h 12m 47s\n",
      "1840:\tlearn: 0.8181618\ttest: 1.1927028\tbest: 1.1918427 (1685)\ttotal: 16m 24s\tremaining: 1h 12m 41s\n",
      "1850:\tlearn: 0.8175513\ttest: 1.1925081\tbest: 1.1918427 (1685)\ttotal: 16m 29s\tremaining: 1h 12m 35s\n",
      "1860:\tlearn: 0.8170342\ttest: 1.1926762\tbest: 1.1918427 (1685)\ttotal: 16m 34s\tremaining: 1h 12m 29s\n",
      "1870:\tlearn: 0.8165197\ttest: 1.1925638\tbest: 1.1918427 (1685)\ttotal: 16m 39s\tremaining: 1h 12m 23s\n",
      "1880:\tlearn: 0.8159226\ttest: 1.1925483\tbest: 1.1918427 (1685)\ttotal: 16m 44s\tremaining: 1h 12m 17s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 1.191842687\n",
      "bestIteration = 1685\n",
      "\n",
      "Shrink model to first 1686 iterations.\n",
      "Train scores:\n",
      "roc_auc_micro\t0.966545\n",
      "roc_auc_1\t0.994760\n",
      "roc_auc_2\t0.960536\n",
      "roc_auc_3\t0.933629\n",
      "roc_auc_4\t0.975682\n",
      "roc_auc_5\t0.947145\n",
      "roc_auc_6\t0.946593\n",
      "roc_auc_7\t0.999566\n",
      "roc_auc_8\t0.973088\n",
      "roc_auc_9\t0.927733\n",
      "roc_auc_10\t0.947651\n",
      "roc_auc_11\t0.880827\n",
      "Validation scores:\n",
      "roc_auc_micro\t0.927759\n",
      "roc_auc_1\t0.932908\n",
      "roc_auc_2\t0.806615\n",
      "roc_auc_3\t0.824422\n",
      "roc_auc_4\t0.939835\n",
      "roc_auc_5\t0.823730\n",
      "roc_auc_6\t0.881343\n",
      "roc_auc_7\t0.000000\n",
      "roc_auc_8\t0.912384\n",
      "roc_auc_9\t0.807402\n",
      "roc_auc_10\t0.831373\n",
      "roc_auc_11\t0.826211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  33,    0,    0,    4,    0,    0,    0,    0,    3,   10,    4],\n",
       "       [   0,    0,    1,    5,    2,    9,    0,    1,   17,   21,   10],\n",
       "       [   1,    0,   34,   16,    0,   10,    0,    0,   93,   14,   73],\n",
       "       [   4,    0,   18,  282,    4,   18,    0,    1,   45,  223,   14],\n",
       "       [   0,    0,   16,   38,    7,   36,    0,    2,   18,   71,   13],\n",
       "       [   0,    0,    0,   45,    2, 2869,    0,   15,  617,  524,  166],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    1,    4,    0,  118,    0,   29,   82,  102,    9],\n",
       "       [   8,    0,   15,   26,    1,  571,    0,    7, 2884,  422,  917],\n",
       "       [   8,    0,    8,  207,    1,  505,    0,    4,  312, 1528,  179],\n",
       "       [   5,    0,   10,   29,    2,  268,    0,    3, 1004,   90, 1243]])"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Start training!\") \n",
    "\n",
    "clf = catboost.CatBoostClassifier(loss_function='MultiClass',\n",
    "                                  verbose=10, iterations=10000, \n",
    "                                  od_type = \"Iter\", od_wait = 200, cat_features=cat_features,\n",
    "                                  eval_metric='MultiClass', max_depth=6)\n",
    "clf.fit(X_train, Y_train, eval_set=(X_val, Y_val))\n",
    "pred_train = clf.predict_proba(X_train)\n",
    "pred_val = clf.predict_proba(X_val)\n",
    "\n",
    "class_val = clf.predict(X_val)\n",
    "train_scores = evaluate(Y_train, pred_train)\n",
    "val_scores = evaluate(Y_val, pred_val)\n",
    "print(\"Train scores:\")\n",
    "for k, v in train_scores.items():\n",
    "    print(\"%s\\t%f\" % (k, v))\n",
    "print(\"Validation scores:\")\n",
    "for k, v in val_scores.items():\n",
    "    print(\"%s\\t%f\" % (k, v))\n",
    "conf_matrix = confusion_matrix(true_val, class_val, labels=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на важность фичей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prev_years_similar_month_amount_торфяной пожарr=2.5', 0.010508118812826314),\n",
       " ('srfpt_1w_mean100', 0.017966383501068238),\n",
       " ('is_autumn', 0.021547257801423882),\n",
       " ('prev_years_similar_month_amount_техногенный пожарr=2.5',\n",
       "  0.024997557488270452),\n",
       " ('trpt_2w_mean500', 0.026731834280032556),\n",
       " ('trpt_2w_mean200', 0.037748477559602936),\n",
       " ('srfpt_2w_mean500', 0.04355880435948699),\n",
       " ('trpp_1w_mean200', 0.04859835549966615),\n",
       " ('prev_years_similar_month_amount_технологический процессr=2.5',\n",
       "  0.04880547468168361),\n",
       " ('srfpt_2w_mean1000', 0.05647599475552961),\n",
       " ('trpt_1w_mean1000', 0.056936502705484476),\n",
       " ('srfpt_1w_mean1000', 0.057282644257477776),\n",
       " ('height_1w_mean200', 0.06291225980421451),\n",
       " ('trpt_2w_mean150', 0.06541959543888888),\n",
       " ('торфяной пожар_r=2.5', 0.07225413415668692),\n",
       " ('trpp_2w_mean1000', 0.07636251549117543),\n",
       " ('srfpt_2w_mean150', 0.07636865312866442),\n",
       " ('trpp_1w_mean500', 0.07779009366471516),\n",
       " ('trpp_3w_mean1000', 0.07834211400243718),\n",
       " ('height_2w_mean150', 0.07966383484075522),\n",
       " ('vwnd_1w_mean200', 0.08173864497178646),\n",
       " ('trpp_2w_mean500', 0.08617678342133829),\n",
       " ('amount_fields_r=1.0', 0.08779395778800263),\n",
       " ('trpt_2w_mean100', 0.08800944072429544),\n",
       " ('height_1w_mean150', 0.09039865278094639),\n",
       " ('vwnd_3w_mean_sfc', 0.09331589220843091),\n",
       " ('uwnd_1w_mean150', 0.10020479604174218),\n",
       " ('trpt_1w_mean100', 0.10158087504369766),\n",
       " ('trpp_1w_mean100', 0.1029252351461396),\n",
       " ('trpt_3w_mean100', 0.10595653488495428),\n",
       " ('uwind 150', 0.10608944052495155),\n",
       " ('field_near_min_dist', 0.106269457333796),\n",
       " ('height_1w_mean500', 0.10639295768908924),\n",
       " ('amount_forests_r=1.0', 0.10784335901710312),\n",
       " ('trpp_3w_mean150', 0.10840364524349033),\n",
       " ('техногенный пожар_r=1.0', 0.10939847529903933),\n",
       " ('vwnd_3w_mean1000', 0.11043364119854575),\n",
       " ('srfpt_3w_mean200', 0.11348200422980594),\n",
       " ('temperature_1w_mean_sfc', 0.11874691081881368),\n",
       " ('height_1w_mean100', 0.11914814540529137),\n",
       " ('srfpt_3w_mean500', 0.12428473756945488),\n",
       " ('trpt_3w_mean1000', 0.12478754991746158),\n",
       " ('trpp_2w_mean100', 0.13133138079329826),\n",
       " ('srfpt_3w_mean1000', 0.1329159124766346),\n",
       " ('srfpt_2w_mean200', 0.13670799867587438),\n",
       " ('temperature_2w_mean 150', 0.13891485265101425),\n",
       " ('технологический процесс_r=2.5', 0.1410717109875224),\n",
       " ('техногенный пожар_r=2.5', 0.14188092545901837),\n",
       " ('temperature_1w_mean 150', 0.1428264315817328),\n",
       " ('temperature 1000', 0.14327705740196256),\n",
       " ('vwnd_1w_mean150', 0.14620766996314588),\n",
       " ('height_3w_mean150', 0.1462313587292476),\n",
       " ('srfpt_1w_mean500', 0.1483406598899317),\n",
       " ('uwnd_3w_mean200', 0.14844864486958168),\n",
       " ('trpp_3w_mean100', 0.14904030152289915),\n",
       " ('temperature_3w_mean 150', 0.15153852724974923),\n",
       " ('height_2w_mean200', 0.1517635090283052),\n",
       " ('temperature_2w_mean 100', 0.15416805518095114),\n",
       " ('vwnd_2w_mean_sfc', 0.1557841367709311),\n",
       " ('day', 0.15753169426096678),\n",
       " ('uwnd_2w_mean200', 0.1589475502033903),\n",
       " ('temperature_1w_mean 100', 0.16088065960860592),\n",
       " ('temperature_2w_mean 500', 0.16133904258427212),\n",
       " ('uwnd_1w_mean200', 0.16231570833569256),\n",
       " ('nature_forest_min_dist', 0.16442898441273565),\n",
       " ('vwnd_2w_mean200', 0.16488515766985973),\n",
       " ('uwnd_1w_mean100', 0.16534897873501042),\n",
       " ('vwnd_2w_mean150', 0.16670404864178354),\n",
       " ('height_3w_mean100', 0.1673254723333948),\n",
       " ('weekday', 0.16847798599972036),\n",
       " ('forest_near_min_dist', 0.16887396737961857),\n",
       " ('контролируемый пал_r=2.5', 0.17013770881912774),\n",
       " ('uwnd_3w_mean100', 0.17302641384053968),\n",
       " ('humidity_1w_mean 500', 0.17588846224510465),\n",
       " ('height_2w_mean100', 0.1760145569162498),\n",
       " ('temperature_3w_mean 500', 0.17641419507858902),\n",
       " ('uwind 100', 0.18097480990423043),\n",
       " ('srfpt_3w_mean150', 0.18298223705254843),\n",
       " ('height_3w_mean200', 0.19018529316537527),\n",
       " ('uwind 200', 0.19024204982616377),\n",
       " ('srfpt_1w_mean200', 0.19071415254475235),\n",
       " ('uwnd_1w_mean500', 0.19250647520543124),\n",
       " ('humidity_2w_mean 500', 0.19382811094943164),\n",
       " ('горение мусора_r=1.0', 0.20307965346368995),\n",
       " ('контролируемый пал_r=1.0', 0.20328121406014318),\n",
       " ('не подтверждено_r=2.5', 0.2036278930171784),\n",
       " ('temperature_sfc', 0.20388955314948382),\n",
       " ('vwnd_1w_mean_sfc', 0.20443620560936282),\n",
       " ('temperature_1w_mean 200', 0.20979262956093062),\n",
       " ('srfpt_3w_mean100', 0.21104631025661652),\n",
       " ('uwnd_3w_mean1000', 0.21228529922555478),\n",
       " ('uwnd_1w_mean1000', 0.21602856543340837),\n",
       " ('temperature_2w_mean 200', 0.21833645032644916),\n",
       " ('vwnd_3w_mean150', 0.2192631415294679),\n",
       " ('min_dist_техногенный пожар', 0.22139077559130557),\n",
       " ('temperature 150', 0.22403120823311923),\n",
       " ('vwnd_2w_mean500', 0.22417462052353782),\n",
       " ('height_2w_mean500', 0.22498755505648577),\n",
       " ('temperature 200', 0.22567783066616265),\n",
       " ('min_dist_торфяной пожар', 0.22927468106897997),\n",
       " ('vwnd_1w_mean500', 0.23192254898761405),\n",
       " ('temperature 500', 0.2328591364293241),\n",
       " ('day_of_month_cos', 0.2361308108793683),\n",
       " ('temperature 100', 0.23816083106561547),\n",
       " ('height_3w_mean500', 0.23913589420853745),\n",
       " ('uwnd_2w_mean1000', 0.24138955608692447),\n",
       " ('temperature_2w_mean_sfc', 0.24433149682394428),\n",
       " ('горение мусора_r=2.5', 0.24434991129271139),\n",
       " ('prev_years_similar_month_amount_горение мусораr=2.5', 0.25129731626729424),\n",
       " ('temperature_3w_mean_sfc', 0.25410041603125416),\n",
       " ('humidity 1000', 0.25554831701955166),\n",
       " ('vwnd_3w_mean500', 0.25638344005939895),\n",
       " ('is_spring', 0.2571917875134201),\n",
       " ('temperature_1w_mean 500', 0.25991528951237675),\n",
       " ('temperature_3w_mean 200', 0.2653163702809828),\n",
       " ('vwnd_2w_mean1000', 0.26760214548404326),\n",
       " ('prev_years_similar_month_amount_сжигание мусораr=2.5', 0.26774556419675594),\n",
       " ('amount_fields_r=2.5', 0.26844431275470915),\n",
       " ('temperature_3w_mean 100', 0.26926630968897897),\n",
       " ('uwind 1000', 0.27040118591420537),\n",
       " ('vwnd_2w_mean100', 0.27266002980930404),\n",
       " ('humidity_3w_mean 1000', 0.2760163906230577),\n",
       " ('uwnd_2w_mean150', 0.2836315936807478),\n",
       " ('uwnd_2w_mean500', 0.2837630741762175),\n",
       " ('vwnd_1w_mean1000', 0.2850723896539932),\n",
       " ('humidity 500', 0.28881154782437657),\n",
       " ('max_dist_сжигание мусора', 0.2966577450861287),\n",
       " ('day_of_month_sin', 0.2976790178682881),\n",
       " ('сжигание мусора_r=1.0', 0.3038696930731045),\n",
       " ('humidity_sfc', 0.30570165905597046),\n",
       " ('amount_forests_r=2.5', 0.3102233236919786),\n",
       " ('uwnd_3w_mean150', 0.3102506698250979),\n",
       " ('height_1w_mean1000', 0.31886486787232465),\n",
       " ('не подтверждено_r=1.0', 0.3191355741340562),\n",
       " ('humidity_2w_mean 1000', 0.32482927415517415),\n",
       " ('prev_years_similar_month_amount_сжигание порубочных остатковr=2.5',\n",
       "  0.32909715685127683),\n",
       " ('uwind 500', 0.3300501774262446),\n",
       " ('surface_height', 0.3384870083579808),\n",
       " ('temperature_3w_mean 1000', 0.3407537196468763),\n",
       " ('vwnd_3w_mean100', 0.35827942649062244),\n",
       " ('humidity_3w_mean 500', 0.3679281720071674),\n",
       " ('uwnd_2w_mean100', 0.3710253278095775),\n",
       " ('vwnd_1w_mean100', 0.37299269183655875),\n",
       " ('latitude', 0.3751642401000393),\n",
       " ('srfpt_1w_mean150', 0.3771440761797192),\n",
       " ('сжигание мусора_r=2.5', 0.38278959293809084),\n",
       " ('vwnd_3w_mean200', 0.38489046650620584),\n",
       " ('лесной пожар_r=2.5', 0.40285093294104835),\n",
       " ('max_dist_технологический процесс', 0.4308731443303647),\n",
       " ('сжигание порубочных остатков_r=2.5', 0.4321960315198398),\n",
       " ('sum_events_all_years_month_in_radius=2.5', 0.43239184098933275),\n",
       " ('лесной пожар_r=1.0', 0.4436377699788029),\n",
       " ('max_dist_техногенный пожар', 0.4461272894307734),\n",
       " ('temperature_1w_mean 1000', 0.4499293158748813),\n",
       " ('srfpt_2w_mean100', 0.4547083999196287),\n",
       " ('cities_near_min_dist', 0.45646467770549165),\n",
       " ('humidity_2w_mean_sfc', 0.4588803267017573),\n",
       " ('prev_years_similar_month_amount_не подтвержденоr=2.5', 0.46006299753559954),\n",
       " ('height_2w_mean1000', 0.4790986521581619),\n",
       " ('humidity_1w_mean 1000', 0.48192314939313696),\n",
       " ('сжигание порубочных остатков_r=1.0', 0.524227093723839),\n",
       " ('humidity_1w_mean_sfc', 0.5280562185973268),\n",
       " ('height_3w_mean1000', 0.5345680882267055),\n",
       " ('humidity_3w_mean_sfc', 0.5465503811431637),\n",
       " ('day_of_year', 0.5548260950096359),\n",
       " ('uwnd_3w_mean500', 0.5784109667016761),\n",
       " ('area_num', 0.5959179245325746),\n",
       " ('max_dist_горение мусора', 0.6377905354458266),\n",
       " ('prev_years_similar_month_amount_контролируемый палr=2.5',\n",
       "  0.6655560845973396),\n",
       " ('amount_cities_r=5', 0.6777351770110653),\n",
       " ('min_dist_сжигание мусора', 0.7869838595305343),\n",
       " ('prev_years_similar_month_amount_природный пожарr=2.5', 0.7897477627140688),\n",
       " ('prev_years_similar_month_amount_лесной пожарr=2.5', 0.826077801098184),\n",
       " ('max_dist_торфяной пожар', 0.8425114331227928),\n",
       " ('amount_cities_r=15', 0.8541557439578782),\n",
       " ('min_dist_горение мусора', 0.8878431959490413),\n",
       " ('longitude', 0.9290474302688085),\n",
       " ('prev_years_similar_month_amount_неконтролируемый палr=2.5',\n",
       "  1.0058009723427075),\n",
       " ('min_dist_технологический процесс', 1.0658561607131698),\n",
       " ('min_dist_лесной пожар', 1.1062047482081008),\n",
       " ('природный пожар_r=1.0', 1.1116109784251733),\n",
       " ('max_dist_сжигание порубочных остатков', 1.1359875191104774),\n",
       " ('min_dist_не подтверждено', 1.1928942537070062),\n",
       " ('day_of_year_sin', 1.2089844694845653),\n",
       " ('min_dist_неконтролируемый пал', 1.2850968255504938),\n",
       " ('temperature_2w_mean 1000', 1.2910923582377705),\n",
       " ('year', 1.327448298954236),\n",
       " ('min_dist_сжигание порубочных остатков', 1.3670127240068208),\n",
       " ('природный пожар_r=2.5', 1.3707313233961262),\n",
       " ('min_dist_контролируемый пал', 1.4477921911371547),\n",
       " ('amount_cities_r=10', 1.4964768793313423),\n",
       " ('day_of_year_cos', 1.586976595452912),\n",
       " ('max_dist_лесной пожар', 1.7240276332837032),\n",
       " ('min_dist_природный пожар', 1.788854100043932),\n",
       " ('max_dist_контролируемый пал', 2.175413094501474),\n",
       " ('month', 2.2173025686288095),\n",
       " ('неконтролируемый пал_r=1.0', 2.2635105514069984),\n",
       " ('неконтролируемый пал_r=2.5', 2.868507698669252),\n",
       " ('subject', 3.1264012840049857),\n",
       " ('max_dist_не подтверждено', 3.1856602562518543),\n",
       " ('district', 3.2349722040850994),\n",
       " ('max_dist_природный пожар', 3.316910201232452),\n",
       " ('max_dist_неконтролируемый пал', 3.625277964389199),\n",
       " ('day_from_start', 6.418106098953122)]"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_scores = {}\n",
    "for score, name in zip(clf.feature_importances_, clf.feature_names_):\n",
    "    feature_scores[name] = score\n",
    "    \n",
    "feature_scores = sorted(feature_scores.items(), key=lambda kv: kv[1])\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5564299544063457"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(class_val.reshape(-1), Y_val) #0.688 0.5707950783836113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_susp_feat = ['events_amount_passed_last_month_in_area_num', 'events_amount_passed_last_month_in_subject', \n",
    "                  'events_amount_passed_last_month_in_district']\n",
    "# drop_susp_feat = []\n",
    "df_features_with_counters_train.drop(columns=drop_susp_feat).to_csv('./solution/df_features_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "по городу (5, 10) и до 3 радиус и ближ расст до города и область - кат фича месяц, область\n",
    "AUC abs max = 0.9272, loss abs min 1.1857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('solution/model.pickle', 'wb') as fout:\n",
    "    pickle.dump(clf, fout, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
